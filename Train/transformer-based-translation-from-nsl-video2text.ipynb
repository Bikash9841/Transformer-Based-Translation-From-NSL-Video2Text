{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-29T06:09:35.077973Z","iopub.status.busy":"2024-08-29T06:09:35.077617Z","iopub.status.idle":"2024-08-29T06:09:35.083518Z","shell.execute_reply":"2024-08-29T06:09:35.082110Z","shell.execute_reply.started":"2024-08-29T06:09:35.077931Z"},"id":"dr3pNcYVdEFv","outputId":"43ec28c8-2f58-4c3b-c90d-0e26b9ffdd0d","trusted":true},"outputs":[],"source":["# !pip install -q pytorchvideo evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-29T06:09:35.085065Z","iopub.status.busy":"2024-08-29T06:09:35.084718Z","iopub.status.idle":"2024-08-29T06:09:35.094946Z","shell.execute_reply":"2024-08-29T06:09:35.094080Z","shell.execute_reply.started":"2024-08-29T06:09:35.085006Z"},"id":"7q8Jg3l-ewNt","outputId":"d310913a-3855-4c12-b21c-3f2845d63ef7","trusted":true},"outputs":[],"source":["# !pip install -q pyarrow==14.0.1"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:35.096331Z","iopub.status.busy":"2024-08-29T06:09:35.095986Z","iopub.status.idle":"2024-08-29T06:09:35.106576Z","shell.execute_reply":"2024-08-29T06:09:35.105752Z","shell.execute_reply.started":"2024-08-29T06:09:35.096288Z"},"id":"4Rw1LiX8CB7i","trusted":true},"outputs":[],"source":["# !pip install -q transformers --upgrade"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:35.109857Z","iopub.status.busy":"2024-08-29T06:09:35.109545Z","iopub.status.idle":"2024-08-29T06:09:35.117092Z","shell.execute_reply":"2024-08-29T06:09:35.116153Z","shell.execute_reply.started":"2024-08-29T06:09:35.109812Z"},"trusted":true},"outputs":[],"source":["# !pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 xformers==0.0.21"]},{"cell_type":"markdown","metadata":{"id":"cTq1OnD0dGTn"},"source":["# Data Collection and Aggregation"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"execution":{"iopub.execute_input":"2024-08-29T06:09:35.118577Z","iopub.status.busy":"2024-08-29T06:09:35.118212Z","iopub.status.idle":"2024-08-29T06:09:35.588083Z","shell.execute_reply":"2024-08-29T06:09:35.587254Z","shell.execute_reply.started":"2024-08-29T06:09:35.118535Z"},"id":"xCaVkbTVX4qI","outputId":"a36b6690-fcc7-4369-be5b-e519e3759469","trusted":true},"outputs":[{"data":{"text/plain":["label\n","म तिम्रो पैसा खान्छु ।             36\n","म लाई भक्तपुर मनपर्छ ।             34\n","तिम्रो काम हरु म लाई छैन ।         34\n","म संग मेरो साथी छ ।                34\n","भक्तपुर मा धेरै काम छ ।            34\n","तिमी हरु मेरो साथी हो ।            34\n","तिमी म लाई मनपर्छ ।                34\n","म घर मा धेरै काम गर्छु ।           33\n","तिम्रो काम छैन पैसा छैन ।          32\n","म अण्डा खान्छु ।                   32\n","मेरो साथी लाई अण्डा मनपर्छ ।       32\n","तिम्रो काम धेरै छ ।                32\n","तिमी संग अण्डा छैन ।               32\n","मेरो धेरै साथी हरु छन् ।           30\n","म लाई अण्डा मनपर्छ ।               28\n","म संग धेरै पैसा छैन ।              26\n","म भक्तपुर मा काम गर्छु ।           18\n","मेरो घर भक्तपुर मा छ ।             18\n","मेरो साथी धेरै भक्तपुर मा छन् ।    16\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import pandas as pd\n","\n","root_path = \"/kaggle/input/nsl-videos\"\n","folder_list = os.listdir(root_path)\n","label_list = [path for path in folder_list if not path.endswith((\".csv\"))]\n","\n","total_df = pd.read_csv('/kaggle/input/trainpath/train.csv')\n","\n","total_df.reset_index(drop = True, inplace = True)\n","total_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"8WeH5kRLX4qJ"},"source":["# Data Splitting"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-29T06:09:35.589417Z","iopub.status.busy":"2024-08-29T06:09:35.589137Z","iopub.status.idle":"2024-08-29T06:09:36.423749Z","shell.execute_reply":"2024-08-29T06:09:36.422814Z","shell.execute_reply.started":"2024-08-29T06:09:35.589387Z"},"id":"_m-QX6C8X4qJ","outputId":"3221ede6-6ecf-43fe-b466-53d8bff199cb","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes: ['तिमी म लाई मनपर्छ ।', 'तिमी संग अण्डा छैन ।', 'तिमी हरु मेरो साथी हो ।', 'तिम्रो काम छैन पैसा छैन ।', 'तिम्रो काम धेरै छ ।', 'तिम्रो काम हरु म लाई छैन ।', 'भक्तपुर मा धेरै काम छ ।', 'म अण्डा खान्छु ।', 'म घर मा धेरै काम गर्छु ।', 'म तिम्रो पैसा खान्छु ।', 'म भक्तपुर मा काम गर्छु ।', 'म लाई अण्डा मनपर्छ ।', 'म लाई भक्तपुर मनपर्छ ।', 'म संग धेरै पैसा छैन ।', 'म संग मेरो साथी छ ।', 'मेरो घर भक्तपुर मा छ ।', 'मेरो धेरै साथी हरु छन् ।', 'मेरो साथी धेरै भक्तपुर मा छन् ।', 'मेरो साथी लाई अण्डा मनपर्छ ।'].\n","Splitted data: 455 114\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","def correct_file_path(file_name: str, root_path: str):\n","    return os.path.join(root_path, file_name)\n","\n","def preprocess_meta_df(df, root_path, label2id):\n","    df.rename(columns={\"video_name\": \"video_path\"}, inplace=True)\n","    df['video_path'] = df['video_path'].apply(lambda x: correct_file_path(x, root_path))\n","#     df['label'] = df['label'].apply(lambda x: label2id[x])\n","    df['label'] = df['label']\n","    \n","    return df\n","\n","train_meta_df, test_meta_df = train_test_split(total_df, test_size=0.2, stratify=total_df['label'], random_state=42)\n","\n","label_list = list(set(train_meta_df['label']))\n","class_labels = sorted(label_list)\n","label2id = {label: i for i, label in enumerate(class_labels)}\n","id2label = {i: label for label, i in label2id.items()}\n","\n","print(f\"Unique classes: {list(label2id.keys())}.\")\n","\n","train_meta_df = preprocess_meta_df(train_meta_df, root_path, label2id)\n","test_meta_df = preprocess_meta_df(test_meta_df, root_path, label2id)\n","\n","print(\"Splitted data:\", len(train_meta_df), len(test_meta_df))"]},{"cell_type":"markdown","metadata":{"id":"0OSo11N6X4qK"},"source":["#  Model Selection and Design"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Configuration for the final Model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:36.425419Z","iopub.status.busy":"2024-08-29T06:09:36.424990Z","iopub.status.idle":"2024-08-29T06:09:38.402280Z","shell.execute_reply":"2024-08-29T06:09:38.401277Z","shell.execute_reply.started":"2024-08-29T06:09:36.425385Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","def get_config():\n","    return {\n","        \"batch_size\": 1,\n","        \"lr\": 10**-4,\n","        \"seq_len\": 15,\n","        \"d_model\": 768,\n","        'lang_tgt':'ne'\n","    }\n","\n","config=get_config()\n","device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-trained Encoder Model (Video Vision Transformer)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-29T06:09:38.404144Z","iopub.status.busy":"2024-08-29T06:09:38.403602Z","iopub.status.idle":"2024-08-29T06:09:54.640317Z","shell.execute_reply":"2024-08-29T06:09:54.639327Z","shell.execute_reply.started":"2024-08-29T06:09:38.404098Z"},"id":"UsdszFloX4qK","outputId":"8de48368-2efd-4503-8255-2fcae2b354a9","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8ce9a3de05f4b868b939f55fc7516ec","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"819f687f43ee4370940820a764d50ae3","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/18.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0e57abf368b437680ff54e8646028d2","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/356M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n","\n","\n","model_checkpoint = \"google/vivit-b-16x2-kinetics400\"\n","image_processor = VivitImageProcessor.from_pretrained(model_checkpoint)\n","\n","# model = VivitForVideoClassification.from_pretrained(model_checkpoint)\n","vivit_model=VivitModel.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:54.642361Z","iopub.status.busy":"2024-08-29T06:09:54.641759Z","iopub.status.idle":"2024-08-29T06:09:54.650414Z","shell.execute_reply":"2024-08-29T06:09:54.649369Z","shell.execute_reply.started":"2024-08-29T06:09:54.642304Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VivitModel(\n","  (embeddings): VivitEmbeddings(\n","    (patch_embeddings): VivitTubeletEmbeddings(\n","      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n","    )\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n","  (encoder): VivitEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x VivitLayer(\n","        (attention): VivitAttention(\n","          (attention): VivitSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): VivitSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): VivitIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (intermediate_act_fn): FastGELUActivation()\n","        )\n","        (output): VivitOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (pooler): VivitPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["vivit_model"]},{"cell_type":"markdown","metadata":{},"source":["## Don't Need the Final Pooler Layer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:54.652032Z","iopub.status.busy":"2024-08-29T06:09:54.651667Z","iopub.status.idle":"2024-08-29T06:09:55.922884Z","shell.execute_reply":"2024-08-29T06:09:55.922011Z","shell.execute_reply.started":"2024-08-29T06:09:54.651988Z"},"trusted":true},"outputs":[],"source":["#Replace pooler layer with the identity function, it just returns what it gets\n","\n","class Identity(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","    def forward(self, x):\n","        return x\n","\n","# model.classifier=Identity()\n","# model.classifier=torch.nn.Linear(768,5)\n","\n","# patch size of 32*32\n","vivit_model.config.tubelet_size=[2,32,32]\n","\n","# 6 encoder block stacks\n","vivit_model.config.num_hidden_layers=12\n","\n","# 8 atttention heads\n","vivit_model.config.num_attention_heads=8\n","\n","# dropout set to 0.1\n","vivit_model.config.hidden_dropout_prob=0.3\n","\n","# number of frames extracting from each video\n","vivit_model.config.num_frames=60\n","\n","vivit_model=VivitModel(vivit_model.config)\n","\n","vivit_model.pooler=Identity()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:55.924403Z","iopub.status.busy":"2024-08-29T06:09:55.924052Z","iopub.status.idle":"2024-08-29T06:09:55.932807Z","shell.execute_reply":"2024-08-29T06:09:55.931864Z","shell.execute_reply.started":"2024-08-29T06:09:55.924361Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VivitModel(\n","  (embeddings): VivitEmbeddings(\n","    (patch_embeddings): VivitTubeletEmbeddings(\n","      (projection): Conv3d(3, 768, kernel_size=(2, 32, 32), stride=(2, 32, 32))\n","    )\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (encoder): VivitEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x VivitLayer(\n","        (attention): VivitAttention(\n","          (attention): VivitSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): VivitSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.3, inplace=False)\n","          )\n","        )\n","        (intermediate): VivitIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (dropout): Dropout(p=0.3, inplace=False)\n","          (intermediate_act_fn): FastGELUActivation()\n","        )\n","        (output): VivitOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.3, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (pooler): Identity()\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["vivit_model"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Decoder Model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:55.934330Z","iopub.status.busy":"2024-08-29T06:09:55.934035Z","iopub.status.idle":"2024-08-29T06:09:55.971754Z","shell.execute_reply":"2024-08-29T06:09:55.970833Z","shell.execute_reply.started":"2024-08-29T06:09:55.934299Z"},"trusted":true},"outputs":[],"source":["import math\n","\n","class InputEmbeddings(torch.nn.Module):\n","\n","    def __init__(self, d_model: int=768, vocab_size: int=27) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.embedding = torch.nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        # (batch, seq_len) --> (batch, seq_len, d_model)\n","        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n","        return self.embedding(x) * math.sqrt(self.d_model)\n","\n","\n","class PositionalEncoding(torch.nn.Module):\n","\n","    def __init__(self, d_model: int=768, seq_len: int=15, dropout: float=0.1) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.dropout = torch.nn.Dropout(dropout)\n","        # Create a matrix of shape (seq_len, d_model)\n","        pe = torch.zeros(seq_len, d_model)\n","        # Create a vector of shape (seq_len)\n","        position = torch.arange(\n","            0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n","        # Create a vector of shape (d_model)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n","        ) * (-math.log(10000.0) / d_model))  # (d_model / 2)\n","        # Apply sine to even indices\n","        # sin(position * (10000 ** (2i / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        # Apply cosine to odd indices\n","        # cos(position * (10000 ** (2i / d_model))\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        # Add a batch dimension to the positional encoding\n","        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n","        # Register the positional encoding as a buffer\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model)\n","        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n","        return self.dropout(x)\n","    \n","    \n","    \n","class MultiHeadAttentionBlock(torch.nn.Module):\n","\n","    def __init__(self, d_model: int=768, h: int=8, dropout: float=0.1) -> None:\n","        super().__init__()\n","        self.d_model = d_model  # Embedding vector size\n","        self.h = h  # Number of heads\n","        # Make sure d_model is divisible by h\n","        assert d_model % h == 0, \"d_model is not divisible by h\"\n","\n","        self.d_k = d_model // h  # Dimension of vector seen by each head\n","        self.w_q = torch.nn.Linear(d_model, d_model, bias=False)  # Wq\n","        self.w_k = torch.nn.Linear(d_model, d_model, bias=False)  # Wk\n","        self.w_v = torch.nn.Linear(d_model, d_model, bias=False)  # Wv\n","        self.w_o = torch.nn.Linear(d_model, d_model, bias=False)  # Wo\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","    @staticmethod\n","    def attention(query, key, value, mask, dropout: torch.nn.Dropout):\n","        d_k = query.shape[-1]\n","        # Just apply the formula from the paper\n","        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n","        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","        if mask is not None:\n","            # Write a very low value (indicating -inf) to the positions where mask == 0\n","            attention_scores.masked_fill_(mask == 0, -1e9)\n","        # (batch, h, seq_len, seq_len) # Apply softmax\n","        attention_scores = attention_scores.softmax(dim=-1)\n","        if dropout is not None:\n","            attention_scores = dropout(attention_scores)\n","        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n","        # return attention scores which can be used for visualization\n","        return (attention_scores @ value), attention_scores\n","\n","    def forward(self, q, k, v, mask):\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        query = self.w_q(q)\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        key = self.w_k(k)\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        value = self.w_v(v)\n","\n","        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n","        query = query.view(\n","            query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n","        key = key.view(key.shape[0], key.shape[1],\n","                       self.h, self.d_k).transpose(1, 2)\n","        value = value.view(\n","            value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n","\n","        # Calculate attention\n","        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n","            query, key, value, mask, self.dropout)\n","        \n","\n","        # Combine all the heads together\n","        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n","        x = x.transpose(1, 2).contiguous().view(\n","            x.shape[0], -1, self.h * self.d_k)\n","\n","        # Multiply by Wo\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        return self.w_o(x)\n","    \n","\n","class LayerNormalization(torch.nn.Module):\n","\n","    def __init__(self, features: int, eps: float = 10**-6) -> None:\n","        super().__init__()\n","        self.eps = eps\n","        # alpha is a learnable parameter\n","        self.alpha = torch.nn.Parameter(torch.ones(features))\n","        # bias is a learnable parameter\n","        self.bias = torch.nn.Parameter(torch.zeros(features))\n","\n","    def forward(self, x):\n","        # x: (batch, seq_len, hidden_size)\n","        # Keep the dimension for broadcasting\n","        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n","        # Keep the dimension for broadcasting\n","        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n","        # eps is to prevent dividing by zero or when std is very small\n","        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n","\n","\n","class FeedForwardBlock(torch.nn.Module):\n","\n","    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n","        super().__init__()\n","        self.linear_1 = torch.nn.Linear(d_model, d_ff)  # w1 and b1\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.linear_2 = torch.nn.Linear(d_ff, d_model)  # w2 and b2\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n","        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n","\n","\n","class ResidualConnection(torch.nn.Module):\n","\n","    def __init__(self, features: int, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.norm = LayerNormalization(features)\n","\n","    # many transformer implmentation also do like this--> normalize the input + positional embedding, then apply mhsa and add skip connection.\n","    # here sublayer is MHSA\n","    def forward(self, x, sublayer):\n","      return x + self.dropout(self.norm(sublayer(x)))\n","\n","\n","\n","class DecoderBlock(torch.nn.Module):\n","\n","    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.cross_attention_block = cross_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = torch.nn.ModuleList(\n","            [ResidualConnection(features, dropout) for _ in range(3)])\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","        x = self.residual_connections[0](\n","            x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n","        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(\n","            x, encoder_output, encoder_output, src_mask))\n","        x = self.residual_connections[2](x, self.feed_forward_block)\n","        return x\n","\n","\n","class Decoder(torch.nn.Module):\n","\n","    def __init__(self, features: int, layers: torch.nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, encoder_output, src_mask, tgt_mask)\n","        return self.norm(x)\n","\n","\n","class ProjectionLayer(torch.nn.Module):\n","\n","    def __init__(self, d_model, vocab_size) -> None:\n","        super().__init__()\n","        self.proj = torch.nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x) -> None:\n","        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n","        return self.proj(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Building Video2Text Transformer Architecture"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:55.976637Z","iopub.status.busy":"2024-08-29T06:09:55.976364Z","iopub.status.idle":"2024-08-29T06:09:55.988087Z","shell.execute_reply":"2024-08-29T06:09:55.987093Z","shell.execute_reply.started":"2024-08-29T06:09:55.976609Z"},"trusted":true},"outputs":[],"source":["class Video2Text(torch.nn.Module):\n","\n","    def __init__(self, encoder, decoder: Decoder, tgt_embed: InputEmbeddings, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n","        super().__init__()\n","        self.video_encoder = encoder\n","        self.decoder = decoder\n","        self.tgt_embed = tgt_embed\n","        self.tgt_pos = tgt_pos\n","        self.projection_layer = projection_layer\n","\n","    def encode(self,src_video):\n","    # (batch,num_frames, num_channels, height, width)\n","        if src_video != None:\n","            perumuted_sample_test_video = src_video.permute(0,2, 1, 3, 4)\n","\n","            inputs = {\n","                \"pixel_values\": perumuted_sample_test_video,\n","            }\n","            # forward pass\n","            outputs = self.video_encoder(**inputs)\n","            \n","#           first token in the sequence is the class token. so, we dont need that. (batchsize, seq_len, embedding)\n","#             return outputs.last_hidden_state[:,1:,:]\n","            return outputs.last_hidden_state[:,:,:]\n","        else:\n","            return None\n","\n","    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n","        # (batch, seq_len, d_model)\n","        tgt = self.tgt_embed(tgt)\n","        tgt = self.tgt_pos(tgt)\n","        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","\n","    def project(self, x):\n","        # (batch, seq_len, vocab_size)\n","        return self.projection_layer(x)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:55.989424Z","iopub.status.busy":"2024-08-29T06:09:55.989114Z","iopub.status.idle":"2024-08-29T06:09:56.002346Z","shell.execute_reply":"2024-08-29T06:09:56.001554Z","shell.execute_reply.started":"2024-08-29T06:09:55.989377Z"},"trusted":true},"outputs":[],"source":["def build_transformer(encoder_model,tgt_vocab_size: int, tgt_seq_len: int, d_model: int = 768, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Video2Text:\n","    \n","    # Create the embedding layers\n","    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n","\n","    # Create the positional encoding layers\n","    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n","\n","    # Create the decoder blocks\n","    decoder_blocks = []\n","    for _ in range(N):\n","        decoder_self_attention_block = MultiHeadAttentionBlock(\n","            d_model, h, dropout)\n","        decoder_cross_attention_block = MultiHeadAttentionBlock(\n","            d_model, h, dropout)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","        decoder_block = DecoderBlock(d_model, decoder_self_attention_block,\n","                                     decoder_cross_attention_block, feed_forward_block, dropout)\n","        decoder_blocks.append(decoder_block)\n","\n","    # Create the encoder and decoder\n","    video_encoder = encoder_model\n","    decoder = Decoder(d_model, torch.nn.ModuleList(decoder_blocks))\n","\n","    # Create the projection layer\n","    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n","\n","    # Create the transformer\n","    transformer = Video2Text(\n","        encoder=video_encoder,decoder=decoder,tgt_embed=tgt_embed, tgt_pos=tgt_pos, projection_layer=projection_layer)\n","\n","    # Initialize the parameters\n","    for p in transformer.decoder.parameters():\n","        if p.dim() > 1:\n","            torch.nn.init.xavier_uniform_(p)\n","\n","\n","    return transformer"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:56.003602Z","iopub.status.busy":"2024-08-29T06:09:56.003297Z","iopub.status.idle":"2024-08-29T06:09:56.016590Z","shell.execute_reply":"2024-08-29T06:09:56.015762Z","shell.execute_reply.started":"2024-08-29T06:09:56.003557Z"},"trusted":true},"outputs":[],"source":["def get_model(config,enc_model,vocab_tgt_len):\n","    v2t_model = build_transformer(encoder_model=enc_model,tgt_vocab_size=vocab_tgt_len,\n","                              tgt_seq_len=config['seq_len'], d_model=config['d_model'])\n","    return v2t_model"]},{"cell_type":"markdown","metadata":{"id":"aiN2WpiMX4qK"},"source":["> # **Apply Necessary Transform and Prepare Dataset**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:56.018385Z","iopub.status.busy":"2024-08-29T06:09:56.017766Z","iopub.status.idle":"2024-08-29T06:09:56.232567Z","shell.execute_reply":"2024-08-29T06:09:56.231587Z","shell.execute_reply.started":"2024-08-29T06:09:56.018324Z"},"id":"NGEtddR_X4qK","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n","  warnings.warn(\n"]}],"source":["import pytorchvideo.data\n","from torch.utils.data import Dataset\n","\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    UniformTemporalSubsample,\n",")\n","\n","from torchvision.transforms import (\n","    Compose,\n","    Lambda,\n","    Resize,\n",")\n","\n","class CustomVideoDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        video_path = row['video_path']\n","        label = row['label']\n","        return video_path, label\n","\n","mean = image_processor.image_mean\n","std = image_processor.image_std\n","\n","if \"shortest_edge\" in image_processor.size:\n","    height = width = image_processor.size[\"shortest_edge\"]\n","else:\n","    height = image_processor.size[\"height\"]\n","    width = image_processor.size[\"width\"]\n","\n","resize_to = (vivit_model.config.image_size, vivit_model.config.image_size)\n","\n","# num_frames_to_sample = model.config.num_frames\n","num_frames_to_sample = 60\n","clip_duration = 10\n","\n","train_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","#                     RandomShortSideScale(min_size=256, max_size=320),\n","                    Resize(resize_to),\n","#                     RandomHorizontalFlip(p=0.5),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","val_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    Resize(resize_to),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","train_custom_dataset = CustomVideoDataset(train_meta_df)\n","train_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in train_custom_dataset]\n","\n","test_custom_dataset = CustomVideoDataset(test_meta_df)\n","test_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in test_custom_dataset]"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"execution":{"iopub.execute_input":"2024-08-29T06:09:56.233834Z","iopub.status.busy":"2024-08-29T06:09:56.233567Z","iopub.status.idle":"2024-08-29T06:09:56.286746Z","shell.execute_reply":"2024-08-29T06:09:56.286032Z","shell.execute_reply.started":"2024-08-29T06:09:56.233804Z"},"id":"Moy-JNAIX4qL","outputId":"db052e2b-041a-4b79-9ca2-25df1f7ebe5a","trusted":true},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","train_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =train_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n","    decode_audio=False,\n","    transform=train_transform,\n",")\n","\n","test_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =test_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n","    decode_audio=False,\n","    transform=val_transform,\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:56.288505Z","iopub.status.busy":"2024-08-29T06:09:56.288110Z","iopub.status.idle":"2024-08-29T06:09:56.301648Z","shell.execute_reply":"2024-08-29T06:09:56.300628Z","shell.execute_reply.started":"2024-08-29T06:09:56.288462Z"},"trusted":true},"outputs":[],"source":["class CustomVideoDataset2(Dataset):\n","\n","    def __init__(self, vdataset, tokenizer_tgt, tgt_lang, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.vdataset = vdataset\n","        self.tokenizer_tgt = tokenizer_tgt\n","        self.tgt_lang = tgt_lang\n","\n","        self.sos_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","#         return len(self.dataframe)\n","        return self.vdataset.num_videos\n","\n","    def __getitem__(self, idx):\n","        \n","        video=next(iter(self.vdataset))['video']\n","#         label=next(iter(self.vdataset))['label']\n","        target_txt=next(iter(self.vdataset))['label']\n","        \n","\n","        # Transform the output text into tokens\n","        dec_input_tokens = self.tokenizer_tgt.encode(target_txt).ids\n","\n","\n","         # We will only add <s> here, and </s> only on the label\n","        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n","\n","#         # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n","        if dec_num_padding_tokens < 0:\n","            raise ValueError(\"Sentence is too long\")\n","\n","\n","#          Add only <s> token\n","        decoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                torch.tensor([self.pad_token] *\n","                             dec_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","#          Add only </eos> token\n","        label = torch.cat(\n","            [\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] *\n","                             dec_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","         )\n","\n","#         # Double check the size of the tensors to make sure they are all seq_len long\n","        assert decoder_input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            \"video\":video,\n","            \"label\":label,\n","            \"decoder_input\":decoder_input,\n","            \"decoder_mask\":(decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n","            \"tgt_text\":target_txt\n","            \n","        }\n","\n","def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0"]},{"cell_type":"markdown","metadata":{},"source":["## Initializing the model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:56.303045Z","iopub.status.busy":"2024-08-29T06:09:56.302732Z","iopub.status.idle":"2024-08-29T06:09:57.107215Z","shell.execute_reply":"2024-08-29T06:09:57.106391Z","shell.execute_reply.started":"2024-08-29T06:09:56.303014Z"},"trusted":true},"outputs":[],"source":["from tokenizers import Tokenizer\n","\n","target_tokenizer=Tokenizer.from_file(str('/kaggle/input/nepalitokenizer/tokenizer_sign_lang_ne.json'))\n","\n","# initialize the model\n","v2t_model=get_model(config=config,enc_model=vivit_model,vocab_tgt_len=target_tokenizer.get_vocab_size())\n","    \n","# v2t_model.to(device)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# **Training Without Huggingface Trainer**"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare and Test Dataloader"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:57.108744Z","iopub.status.busy":"2024-08-29T06:09:57.108413Z","iopub.status.idle":"2024-08-29T06:09:57.114869Z","shell.execute_reply":"2024-08-29T06:09:57.113885Z","shell.execute_reply.started":"2024-08-29T06:09:57.108710Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader, random_split\n","\n","new_train_dataset=CustomVideoDataset2(train_dataset,target_tokenizer,config['lang_tgt'],config['seq_len'])\n","new_val_dataset=CustomVideoDataset2(test_dataset,target_tokenizer,config['lang_tgt'],config['seq_len'])\n","\n","train_dataloader = DataLoader(new_train_dataset, batch_size=1,shuffle=True)\n","val_dataloader = DataLoader(new_val_dataset, batch_size=1,shuffle=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:57.116679Z","iopub.status.busy":"2024-08-29T06:09:57.116289Z","iopub.status.idle":"2024-08-29T06:09:57.128933Z","shell.execute_reply":"2024-08-29T06:09:57.127998Z","shell.execute_reply.started":"2024-08-29T06:09:57.116647Z"},"trusted":true},"outputs":[],"source":["# i=0\n","# for data in train_dataloader:\n","#     i+=1\n","#     if i>3:\n","#         break\n","#     print(data['tgt_text'],data['label'])\n"]},{"cell_type":"markdown","metadata":{},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:57.131688Z","iopub.status.busy":"2024-08-29T06:09:57.131364Z","iopub.status.idle":"2024-08-29T06:09:57.149653Z","shell.execute_reply":"2024-08-29T06:09:57.148712Z","shell.execute_reply.started":"2024-08-29T06:09:57.131656Z"},"trusted":true},"outputs":[],"source":["def greedy_decode(model, src_video, source_mask, tokenizer_tgt, max_len, device):\n","    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n","    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n","\n","    # Precompute the encoder output and reuse it for every step\n","    encoder_output = model.encode(src_video=src_video)\n","#     encoder_output = (torch.randint(2,7,(1,784,768))).type_as(encoder_output).to(device)\n","    \n","#     print(f'encoder_output: {encoder_output[:,392:400,:20]}')\n","    # Initialize the decoder input with the sos token\n","    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(src_video.type(torch.LongTensor)).to(device)\n","    \n","#     print(f\"decoder input: {decoder_input,decoder_input.shape}\")\n","    while True:\n","        if decoder_input.size(1) == max_len:\n","            break\n","\n","        # build mask for target\n","        decoder_mask = causal_mask(decoder_input.size(\n","            1)).type_as(src_video.type(torch.LongTensor)).to(device)\n","        \n","        \n","#         print(f'decoder mask: {decoder_mask,decoder_mask.shape}')\n","\n","        # calculate output\n","        out = model.decode(encoder_output=encoder_output, src_mask=None,tgt=decoder_input, tgt_mask=decoder_mask)\n","\n","        # get next token\n","        prob = model.project(out[:, -1])\n","        \n","        _, next_word = torch.max(prob, dim=1)\n","        decoder_input = torch.cat(\n","            [decoder_input, torch.empty(1, 1).type_as(src_video.type(torch.LongTensor)).fill_(next_word.item()).to(device)], dim=1\n","        )\n","        \n","#         print(f'next_word: {next_word}')\n","        if next_word == eos_idx:\n","            break\n","\n","    return decoder_input.squeeze(0)\n","\n","\n","# model, src_video, source_mask, tokenizer_tgt, max_len, device\n","def beam_search_decode(model, beam_size, src_video, source_mask, tokenizer_tgt, max_len, device):\n","    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n","    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n","\n","    # Precompute the encoder output and reuse it for every step\n","    encoder_output = model.encode(src_video=src_video)\n","    # Initialize the decoder input with the sos token\n","    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(src_video.type(torch.LongTensor)).to(device)\n","\n","    # Create a candidate list\n","    candidates = [(decoder_initial_input, 1)]\n","\n","    while True:\n","\n","        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n","        if any([cand.size(1) == max_len for cand, _ in candidates]):\n","            break\n","\n","        # Create a new list of candidates\n","        new_candidates = []\n","\n","        for candidate, score in candidates:\n","\n","            # Do not expand candidates that have reached the eos token\n","            if candidate[0][-1].item() == eos_idx:\n","                continue\n","\n","            # Build the candidate's mask\n","            candidate_mask = causal_mask(candidate.size(1)).type_as(src_video.type(torch.LongTensor)).to(device)\n","            \n","            # calculate output\n","            out = model.decode(encoder_output=encoder_output, src_mask=None, tgt=candidate, tgt_mask=candidate_mask)\n","            \n","            # get next token probabilities\n","            prob = model.project(out[:, -1])\n","            \n","            # get the top k candidates\n","            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n","            \n","            for i in range(beam_size):\n","                # for each of the top k candidates, get the token and its probability\n","                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n","                token_prob = topk_prob[0][i].item()\n","                # create a new candidate by appending the token to the current candidate\n","                new_candidate = torch.cat([candidate, token], dim=1)\n","                # We sum the log probabilities because the probabilities are in log space\n","                new_candidates.append((new_candidate, score + token_prob))\n","\n","        # Sort the new candidates by their score\n","        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n","        # Keep only the top k candidates\n","        candidates = candidates[:beam_size]\n","\n","        # If all the candidates have reached the eos token, stop\n","        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n","            break\n","\n","    # Return the best candidate\n","    return candidates[0][0].squeeze()\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:57.151043Z","iopub.status.busy":"2024-08-29T06:09:57.150736Z","iopub.status.idle":"2024-08-29T06:09:57.476625Z","shell.execute_reply":"2024-08-29T06:09:57.475816Z","shell.execute_reply.started":"2024-08-29T06:09:57.151012Z"},"trusted":true},"outputs":[],"source":["import torchmetrics\n","\n","def run_validation(model, validation_ds, tokenizer_tgt, max_len, device, print_msg, num_examples=2):\n","    model.eval()\n","    count = 0\n","\n","    try:\n","        # get the console window width\n","        with os.popen('stty size', 'r') as console:\n","            _, console_width = console.read().split()\n","            console_width = int(console_width)\n","    except:\n","        # If we can't get the console width, use 80 as default\n","        console_width = 80\n","\n","    with torch.no_grad():\n","        for batch in validation_ds:\n","            count += 1\n","            encoder_input = batch[\"video\"].to(device)  # (b, seq_len)\n","\n","            # check that the batch size is 1\n","            assert encoder_input.size(\n","                0) == 1, \"Batch size must be 1 for validation\"\n","\n","            model_out_greedy = greedy_decode(\n","                model, encoder_input, None, tokenizer_tgt, max_len, device)\n","            model_out_beam = beam_search_decode(model, 3, encoder_input, None, tokenizer_tgt, max_len, device)\n","\n","            target_text = batch[\"tgt_text\"][0]\n","            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n","            model_out_text_greedy = tokenizer_tgt.decode(model_out_greedy.detach().cpu().numpy())\n","\n","\n","            # Print the target and model output\n","            if count <4:\n","                print_msg(f\"{f'TARGET: ':>20}{target_text}\")\n","                print_msg(f\"{f'PREDICTED GREEDY: ':>20}{model_out_text_greedy}\")\n","                print_msg(f\"{f'PREDICTED BEAM: ':>20}{model_out_text_beam}\")\n","\n","            # print(count)\n","            if count == num_examples:\n","                print_msg('-'*console_width)\n","                break"]},{"cell_type":"markdown","metadata":{},"source":["## Load the saved model if notebook restarted"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:09:57.477877Z","iopub.status.busy":"2024-08-29T06:09:57.477605Z","iopub.status.idle":"2024-08-29T06:10:10.752214Z","shell.execute_reply":"2024-08-29T06:10:10.751116Z","shell.execute_reply.started":"2024-08-29T06:09:57.477846Z"},"trusted":true},"outputs":[],"source":["saved_model=torch.load(\"/kaggle/input/saved-model2/69_mtrain.pt\")"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:10.753837Z","iopub.status.busy":"2024-08-29T06:10:10.753514Z","iopub.status.idle":"2024-08-29T06:10:10.760517Z","shell.execute_reply":"2024-08-29T06:10:10.759532Z","shell.execute_reply.started":"2024-08-29T06:10:10.753803Z"},"trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['model_state_dict', 'optimizer_state_dict', 'train_loss', 'val_loss', 't_wer', 'v_wer', 'epoch'])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["saved_model.keys()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:10.762598Z","iopub.status.busy":"2024-08-29T06:10:10.761907Z","iopub.status.idle":"2024-08-29T06:10:10.785968Z","shell.execute_reply":"2024-08-29T06:10:10.785128Z","shell.execute_reply.started":"2024-08-29T06:10:10.762553Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 1471, 768])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["saved_model['model_state_dict']['video_encoder.embeddings.position_embeddings'].shape"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:10.787314Z","iopub.status.busy":"2024-08-29T06:10:10.787011Z","iopub.status.idle":"2024-08-29T06:10:10.935241Z","shell.execute_reply":"2024-08-29T06:10:10.934281Z","shell.execute_reply.started":"2024-08-29T06:10:10.787282Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["v2t_model.load_state_dict(saved_model['model_state_dict'])"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:10.936760Z","iopub.status.busy":"2024-08-29T06:10:10.936457Z","iopub.status.idle":"2024-08-29T06:10:11.092700Z","shell.execute_reply":"2024-08-29T06:10:11.091793Z","shell.execute_reply.started":"2024-08-29T06:10:10.936729Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Video2Text(\n","  (video_encoder): VivitModel(\n","    (embeddings): VivitEmbeddings(\n","      (patch_embeddings): VivitTubeletEmbeddings(\n","        (projection): Conv3d(3, 768, kernel_size=(2, 32, 32), stride=(2, 32, 32))\n","      )\n","      (dropout): Dropout(p=0.3, inplace=False)\n","    )\n","    (encoder): VivitEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x VivitLayer(\n","          (attention): VivitAttention(\n","            (attention): VivitSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): VivitSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.3, inplace=False)\n","            )\n","          )\n","          (intermediate): VivitIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (dropout): Dropout(p=0.3, inplace=False)\n","            (intermediate_act_fn): FastGELUActivation()\n","          )\n","          (output): VivitOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.3, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    (pooler): Identity()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0-5): 6 x DecoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=768, out_features=768, bias=False)\n","          (w_k): Linear(in_features=768, out_features=768, bias=False)\n","          (w_v): Linear(in_features=768, out_features=768, bias=False)\n","          (w_o): Linear(in_features=768, out_features=768, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (cross_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=768, out_features=768, bias=False)\n","          (w_k): Linear(in_features=768, out_features=768, bias=False)\n","          (w_v): Linear(in_features=768, out_features=768, bias=False)\n","          (w_o): Linear(in_features=768, out_features=768, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=768, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=2048, out_features=768, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-2): 3 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (tgt_embed): InputEmbeddings(\n","    (embedding): Embedding(27, 768)\n","  )\n","  (tgt_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (projection_layer): ProjectionLayer(\n","    (proj): Linear(in_features=768, out_features=27, bias=True)\n","  )\n",")"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["v2t_model.to(device)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:11.094286Z","iopub.status.busy":"2024-08-29T06:10:11.094017Z","iopub.status.idle":"2024-08-29T06:10:11.100873Z","shell.execute_reply":"2024-08-29T06:10:11.099900Z","shell.execute_reply.started":"2024-08-29T06:10:11.094257Z"},"trusted":true},"outputs":[{"data":{"text/plain":["70"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["saved_model['epoch']"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:11.102812Z","iopub.status.busy":"2024-08-29T06:10:11.102051Z","iopub.status.idle":"2024-08-29T06:10:11.110767Z","shell.execute_reply":"2024-08-29T06:10:11.109847Z","shell.execute_reply.started":"2024-08-29T06:10:11.102768Z"},"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# plt.plot(saved_model['train_loss'])\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T06:10:11.112609Z","iopub.status.busy":"2024-08-29T06:10:11.112199Z","iopub.status.idle":"2024-08-29T08:38:57.500508Z","shell.execute_reply":"2024-08-29T08:38:57.499538Z","shell.execute_reply.started":"2024-08-29T06:10:11.112567Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing Epoch 70:   0%|          | 0/455 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","Processing Epoch 70: 100%|██████████| 455/455 [06:21<00:00,  1.19it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म घर मा धेरै काम गर्छु ।\n","  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । धेरै पैसा छैन । धेरै छ ।\n","            TARGET: मेरो साथी धेरै भक्तपुर मा छन् ।\n","  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । धेरै पैसा छैन । धेरै छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 70 | Training Loss: 1.033      Validation Loss: 1.036             Train WER: 0.502      Validation WER: 0.496\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 71: 100%|██████████| 455/455 [06:23<00:00,  1.19it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिम्रो काम हरु म लाई छैन ।\n","  PREDICTED GREEDY: म अण्डा खान्छु ।\n","    PREDICTED BEAM: तिमी म लाई मनपर्छ । भक्तपुर मा काम गर्छु मनपर्छ । गर्छु ।\n","            TARGET: भक्तपुर मा धेरै काम छ ।\n","  PREDICTED GREEDY: म अण्डा खान्छु ।\n","    PREDICTED BEAM: तिमी म लाई मनपर्छ । भक्तपुर मा काम गर्छु मनपर्छ । गर्छु ।\n","--------------------------------------------------------------------------------\n"," Epoch: 71 | Training Loss: 1.031      Validation Loss: 1.036             Train WER: 0.493      Validation WER: 0.485\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 72: 100%|██████████| 455/455 [06:13<00:00,  1.22it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिमी हरु मेरो साथी हो ।\n","  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन । लाई छैन ।\n","            TARGET: म संग मेरो साथी छ ।\n","  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन । लाई छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 72 | Training Loss: 1.029      Validation Loss: 1.033             Train WER: 0.504      Validation WER: 0.536\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 73: 100%|██████████| 455/455 [06:28<00:00,  1.17it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिम्रो काम हरु म लाई छैन ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । छैन । छैन । छैन ।\n","            TARGET: म संग धेरै पैसा छैन ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । छैन । छैन । छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 73 | Training Loss: 1.026      Validation Loss: 1.026             Train WER: 0.485      Validation WER: 0.472\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 74: 100%|██████████| 455/455 [06:29<00:00,  1.17it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिमी संग अण्डा छैन ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । काम छैन । छ । छैन ।\n","            TARGET: मेरो घर भक्तपुर मा छ ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । काम छैन । छ । छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 74 | Training Loss: 1.026      Validation Loss: 1.029             Train WER: 0.494      Validation WER: 0.492\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 75: 100%|██████████| 455/455 [06:13<00:00,  1.22it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिम्रो काम धेरै छ ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । काम धेरै छ । छन् छैन ।\n","            TARGET: म लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । काम धेरै छ । छन् छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 75 | Training Loss: 1.025      Validation Loss: 1.039             Train WER: 0.493      Validation WER: 0.489\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 76: 100%|██████████| 455/455 [06:07<00:00,  1.24it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म तिम्रो पैसा खान्छु ।\n","  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n","    PREDICTED BEAM: मेरो साथी लाई अण्डा मनपर्छ । अण्डा मनपर्छ । घर भक्तपुर मनपर्छ ।\n","            TARGET: म संग मेरो साथी छ ।\n","  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n","    PREDICTED BEAM: मेरो साथी लाई अण्डा मनपर्छ । अण्डा मनपर्छ । घर भक्तपुर मनपर्छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 76 | Training Loss: 1.031      Validation Loss: 1.026             Train WER: 0.496      Validation WER: 0.505\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 77: 100%|██████████| 455/455 [05:52<00:00,  1.29it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म घर मा धेरै काम गर्छु ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिमी हरु मेरो साथी हो । धेरै भक्तपुर मा छन् । छ ।\n","            TARGET: म संग धेरै पैसा छैन ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । छैन । छैन । छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 77 | Training Loss: 1.027      Validation Loss: 1.034             Train WER: 0.494      Validation WER: 0.511\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 78: 100%|██████████| 455/455 [06:03<00:00,  1.25it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिम्रो काम धेरै छ ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । संग मेरो साथी छ । संग धेरै\n","            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । संग मेरो साथी छ । छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 78 | Training Loss: 1.027      Validation Loss: 1.033             Train WER: 0.488      Validation WER: 0.508\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 79: 100%|██████████| 455/455 [05:55<00:00,  1.28it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । छैन । छैन । छैन ।\n","            TARGET: तिमी संग अण्डा छैन ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । छैन । छैन । छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 79 | Training Loss: 1.028      Validation Loss: 1.027             Train WER: 0.498      Validation WER: 0.454\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 80: 100%|██████████| 455/455 [06:03<00:00,  1.25it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । अण्डा छैन । खान्छु छ । खान्छु ।\n","            TARGET: मेरो घर भक्तपुर मा छ ।\n","  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । अण्डा छैन । खान्छु छ । खान्छु ।\n","--------------------------------------------------------------------------------\n"," Epoch: 80 | Training Loss: 1.027      Validation Loss: 1.033             Train WER: 0.493      Validation WER: 0.498\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 81: 100%|██████████| 455/455 [05:48<00:00,  1.31it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिमी हरु मेरो साथी हो ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । साथी हो । गर्छु । गर्छु ।\n","            TARGET: भक्तपुर मा धेरै काम छ ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । साथी हो । गर्छु । गर्छु ।\n","--------------------------------------------------------------------------------\n"," Epoch: 81 | Training Loss: 1.029      Validation Loss: 1.038             Train WER: 0.512      Validation WER: 0.471\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 82: 100%|██████████| 455/455 [05:41<00:00,  1.33it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म लाई भक्तपुर मनपर्छ ।\n","  PREDICTED GREEDY: म अण्डा खान्छु ।\n","    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । धेरै भक्तपुर मा छन् । छ ।\n","            TARGET: तिम्रो काम धेरै छ ।\n","  PREDICTED GREEDY: म अण्डा खान्छु ।\n","    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । धेरै भक्तपुर मा छन् । छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 82 | Training Loss: 1.034      Validation Loss: 1.028             Train WER: 0.505      Validation WER: 0.491\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 83: 100%|██████████| 455/455 [05:37<00:00,  1.35it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिम्रो काम छैन पैसा छैन ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिमी हरु मेरो साथी हो । काम हरु छन् । लाई मनपर्छ ।\n","            TARGET: तिमी हरु मेरो साथी हो ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिमी हरु मेरो साथी हो । काम हरु छन् । लाई मनपर्छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 83 | Training Loss: 1.029      Validation Loss: 1.031             Train WER: 0.495      Validation WER: 0.508\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 84: 100%|██████████| 455/455 [05:58<00:00,  1.27it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: म संग मेरो साथी छ । भक्तपुर मा काम गर्छु । मेरो साथी छ\n","            TARGET: तिमी हरु मेरो साथी हो ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: म संग मेरो साथी छ । भक्तपुर मा काम गर्छु । मेरो साथी छ\n","--------------------------------------------------------------------------------\n"," Epoch: 84 | Training Loss: 1.022      Validation Loss: 1.036             Train WER: 0.475      Validation WER: 0.508\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 85: 100%|██████████| 455/455 [06:11<00:00,  1.23it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म घर मा धेरै काम गर्छु ।\n","  PREDICTED GREEDY: म अण्डा खान्छु ।\n","    PREDICTED BEAM: म लाई अण्डा मनपर्छ । छैन अण्डा मनपर्छ । छैन अण्डा मनपर्छ ।\n","            TARGET: म भक्तपुर मा काम गर्छु ।\n","  PREDICTED GREEDY: म अण्डा खान्छु ।\n","    PREDICTED BEAM: म लाई अण्डा मनपर्छ । छैन अण्डा मनपर्छ । छैन अण्डा मनपर्छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 85 | Training Loss: 1.03      Validation Loss: 1.032             Train WER: 0.519      Validation WER: 0.483\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 86: 100%|██████████| 455/455 [05:57<00:00,  1.27it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिमी हरु मेरो साथी हो । लाई अण्डा मनपर्छ । लाई मनपर्छ ।\n","            TARGET: तिमी म लाई मनपर्छ ।\n","  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । छैन । छैन । छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 86 | Training Loss: 1.026      Validation Loss: 1.021             Train WER: 0.494      Validation WER: 0.45\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 87: 100%|██████████| 455/455 [06:15<00:00,  1.21it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: म लाई भक्तपुर मनपर्छ ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । साथी हो । संग मेरो साथी छ\n","            TARGET: तिमी संग अण्डा छैन ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । संग मेरो साथी छ । छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 87 | Training Loss: 1.029      Validation Loss: 1.039             Train WER: 0.475      Validation WER: 0.495\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 88: 100%|██████████| 455/455 [06:25<00:00,  1.18it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । घर मा धेरै काम गर्छु । छैन ।\n","            TARGET: तिमी हरु मेरो साथी हो ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । घर मा धेरै काम गर्छु । छैन ।\n","--------------------------------------------------------------------------------\n"," Epoch: 88 | Training Loss: 1.025      Validation Loss: 1.031             Train WER: 0.487      Validation WER: 0.462\n"]},{"name":"stderr","output_type":"stream","text":["Processing Epoch 89: 100%|██████████| 455/455 [06:13<00:00,  1.22it/s]\n","stty: 'standard input': Inappropriate ioctl for device\n"]},{"name":"stdout","output_type":"stream","text":["            TARGET: तिम्रो काम हरु म लाई छैन ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिमी हरु मेरो साथी हो । लाई अण्डा मनपर्छ । अण्डा मनपर्छ ।\n","            TARGET: म तिम्रो पैसा खान्छु ।\n","  PREDICTED GREEDY: म संग मेरो साथी छ ।\n","    PREDICTED BEAM: तिमी हरु मेरो साथी हो । लाई अण्डा मनपर्छ । अण्डा मनपर्छ ।\n","--------------------------------------------------------------------------------\n"," Epoch: 89 | Training Loss: 1.027      Validation Loss: 1.028             Train WER: 0.49      Validation WER: 0.496\n"]}],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","# optimizer = torch.optim.Adam(v2t_model.parameters(), lr=10**-2, eps=1e-9)\n","optimizer = torch.optim.Adam(v2t_model.parameters(), lr=1e-5, eps=1e-9, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","optimizer.load_state_dict(saved_model['optimizer_state_dict'])\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=target_tokenizer.token_to_id(\n","        '[PAD]'),label_smoothing=0.1).to(device)\n","\n","\n","\n","# loss_list_train=[]\n","# loss_list_val=[]\n","# wer_list_train=[]\n","# wer_list_val=[]\n","\n","loss_list_train=saved_model['train_loss']\n","loss_list_val=saved_model['val_loss']\n","wer_list_train=saved_model['t_wer']\n","wer_list_val=saved_model['v_wer']\n","\n","# for epoch in range(30):\n","for epoch in range(saved_model['epoch'],90):\n","    torch._C._cuda_emptyCache()\n","    v2t_model.train()\n","    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n","    \n","    #accumulate LOSS and WER\n","    acc_loss=0\n","    acc_wer=0\n","    for batch in batch_iterator:\n","        \n","                src_video=batch['video'].to(device)\n","                decoder_input=batch['decoder_input'].to(device)\n","                decoder_mask=batch['decoder_mask'].to(device)\n","                \n","                 # Run the tensors through the encoder, decoder and the projection layer\n","                encoder_output = v2t_model.encode(src_video=src_video)  # (B, seq_len, d_model)\n","                decoder_output = v2t_model.decode(encoder_output=encoder_output, src_mask=None,tgt=decoder_input, tgt_mask=decoder_mask) # (B, seq_len, d_model)\n","                 # (B, seq_len, vocab_size)\n","                proj_output = v2t_model.project(decoder_output)\n","                \n","                 # Compare the output with the label\n","                label = batch['label'].to(device)  # (B, seq_len)\n","\n","                 # Compute the loss using a simple cross entropy\n","                loss = loss_fn(proj_output.view(-1, target_tokenizer.get_vocab_size()), label.view(-1))\n","                \n","                 # accumulated loss for every batch in a single epoch\n","                acc_loss+=loss.item()\n","                \n","        \n","                # Backpropagate the loss\n","                loss.backward()\n","            \n","\n","                # Update the weights\n","                optimizer.step()\n","                optimizer.zero_grad(set_to_none=True)\n","                \n","                #calculating training WER\n","                pred_tokens = torch.argmax(proj_output, dim=-1)  # Get the predicted token indices\n","                pred_sentences = target_tokenizer.decode(pred_tokens.detach().cpu().numpy()[0][:8], skip_special_tokens=True)\n","                metric = torchmetrics.text.WordErrorRate()\n","                t_WER=metric(pred_sentences, batch['tgt_text'])\n","                acc_wer+=t_WER\n","                \n","                \n","    \n","    \n","    loss_list_train.append(np.round(acc_loss/len(train_dataloader),3))\n","    wer_list_train.append(np.round(acc_wer.numpy()/len(train_dataloader),3))\n","    \n","    # predict sentences\n","    run_validation(v2t_model,val_dataloader, target_tokenizer, 15 ,device,lambda msg: batch_iterator.write(msg))\n","    \n","\n","    v2t_model.eval()\n","    acc_loss=0\n","    acc_wer=0\n","    \n","    with torch.no_grad():\n","        for batchv in val_dataloader:\n","\n","            src_video=batchv['video'].to(device)\n","            decoder_input=batchv['decoder_input'].to(device)\n","            decoder_mask=batchv['decoder_mask'].to(device)\n","\n","            # Run the tensors through the encoder, decoder and the projection layer\n","            encoder_output = v2t_model.encode(src_video=src_video)  # (B, seq_len, d_model)\n","            decoder_output = v2t_model.decode(encoder_output=encoder_output, src_mask=None,tgt=decoder_input, tgt_mask=decoder_mask) # (B, seq_len, d_model)\n","            # (B, seq_len, vocab_size)\n","            proj_output = v2t_model.project(decoder_output)\n","\n","            # Compare the output with the label\n","            label = batchv['label'].to(device)  # (B, seq_len)\n","\n","            # Compute the loss using a simple cross entropy\n","            val_loss = loss_fn(proj_output.view(-1, target_tokenizer.get_vocab_size()), label.view(-1))\n","\n","            acc_loss+=val_loss.item()\n","\n","            #calculating validation WER\n","            pred_tokens = torch.argmax(proj_output, dim=-1)  # Get the predicted token indices\n","            pred_sentences = target_tokenizer.decode(pred_tokens.detach().cpu().numpy()[0][:8], skip_special_tokens=True)\n","            metric = torchmetrics.text.WordErrorRate()\n","            v_WER=metric(pred_sentences, batchv['tgt_text'])\n","            acc_wer+=v_WER\n","            \n","        \n","    with torch.no_grad():   \n","        loss_list_val.append(np.round(acc_loss/len(val_dataloader),3))\n","        wer_list_val.append(np.round(acc_wer.numpy()/len(val_dataloader),3))\n","    \n","    if (epoch+1)%5==0:\n","        \n","        torch.save({\"model_state_dict\":v2t_model.state_dict(),\n","                    \"optimizer_state_dict\":optimizer.state_dict(),\n","                    \"train_loss\":loss_list_train,\n","                    \"val_loss\":loss_list_val,\n","                   \"t_wer\":wer_list_train,\n","                    \"v_wer\":wer_list_val,\n","                    \"epoch\":epoch+1,\n","                    },\n","                    f\"{epoch}_32X32mtrain.pt\")\n","        \n","    \n","    print(f\" Epoch: {epoch} | Training Loss: {loss_list_train[-1]}      Validation Loss: {loss_list_val[-1]}\\\n","             Train WER: {wer_list_train[-1]}      Validation WER: {wer_list_val[-1]}\")\n","    \n","                "]},{"cell_type":"markdown","metadata":{},"source":["# **Running Tests**"]},{"cell_type":"markdown","metadata":{},"source":["## Plot Each Frames Extracted from the Video"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","video=next(iter(val_dataloader))\n","\n","# Create subplots\n","fig, axs = plt.subplots(8, 8, figsize=(10, 10))\n","\n","# Plot images\n","f=0\n","for i in range(8):\n","    for j in range(8):\n","        f+=1\n","        if f<60:\n","          im=video['video'].permute(0,2,3,4,1)[0,f,:,:,:]\n","          axs[i, j].imshow(im)\n","        axs[i, j].set_title(f'frame: {f+1}')\n","        axs[i, j].axis('off')  # Hide axis\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# **Inference on Single Video**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# saved_model=torch.load('/kaggle/working/199_mtrain.pt')\n","# v2t_model=saved_model['model'].to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1qsy-kql_AT","trusted":true},"outputs":[],"source":["def run_inference(model, video, tokenizer_tgt, max_len, device):\n","    model.eval()\n","\n","    source_texts = []\n","    expected = []\n","    predicted = []\n","\n","    with torch.no_grad():\n","\n","        encoder_input = video['video'][0].unsqueeze(0).to(device)  # (b, seq_len)\n","\n","        # check that the batch size is 1\n","        assert encoder_input.size(\n","            0) == 1, \"Batch size must be 1 for validation\"\n","\n","        model_out = greedy_decode(\n","            model, encoder_input, None, tokenizer_tgt, max_len, device)\n","\n","        target_text = video[\"tgt_text\"][0]\n","        model_out_text = tokenizer_tgt.decode(\n","            model_out.detach().cpu().numpy())\n","\n","#             source_texts.append(source_text)\n","        expected.append(target_text)\n","        predicted.append(model_out_text)\n","\n","        # Print the source, target and model output\n","        print('-----------------------------')\n","        print(f\"TARGET: {target_text}\")\n","        print(f\"PREDICTED: {model_out_text}\")\n","\n","        # Compute the word error rate   \n","        metric = torchmetrics.WordErrorRate()\n","        wer = metric(predicted, expected)\n","        print(f\"Word Error Rate:{wer}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z86pUo8gmys6","trusted":true},"outputs":[],"source":["video=next(iter(train_dataloader))\n","run_inference(model=v2t_model, video=video, tokenizer_tgt=target_tokenizer, max_len=15, device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5594249,"sourceId":9247474,"sourceType":"datasetVersion"},{"datasetId":5594259,"sourceId":9247489,"sourceType":"datasetVersion"},{"datasetId":5602855,"sourceId":9259878,"sourceType":"datasetVersion"},{"datasetId":5606623,"sourceId":9265147,"sourceType":"datasetVersion"},{"datasetId":5610744,"sourceId":9271393,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
