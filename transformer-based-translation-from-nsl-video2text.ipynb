{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9247474,"sourceType":"datasetVersion","datasetId":5594249},{"sourceId":9247489,"sourceType":"datasetVersion","datasetId":5594259},{"sourceId":9247499,"sourceType":"datasetVersion","datasetId":5594269}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q pytorchvideo evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dr3pNcYVdEFv","outputId":"43ec28c8-2f58-4c3b-c90d-0e26b9ffdd0d","execution":{"iopub.status.busy":"2024-08-26T16:29:09.428884Z","iopub.execute_input":"2024-08-26T16:29:09.429325Z","iopub.status.idle":"2024-08-26T16:29:09.434673Z","shell.execute_reply.started":"2024-08-26T16:29:09.429279Z","shell.execute_reply":"2024-08-26T16:29:09.433672Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# pip install -q pyarrow==14.0.1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7q8Jg3l-ewNt","outputId":"d310913a-3855-4c12-b21c-3f2845d63ef7","execution":{"iopub.status.busy":"2024-08-26T16:29:09.436318Z","iopub.execute_input":"2024-08-26T16:29:09.436616Z","iopub.status.idle":"2024-08-26T16:29:09.444980Z","shell.execute_reply.started":"2024-08-26T16:29:09.436585Z","shell.execute_reply":"2024-08-26T16:29:09.444063Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# pip install -q transformers --upgrade","metadata":{"id":"4Rw1LiX8CB7i","execution":{"iopub.status.busy":"2024-08-26T16:29:09.449115Z","iopub.execute_input":"2024-08-26T16:29:09.449403Z","iopub.status.idle":"2024-08-26T16:29:09.455696Z","shell.execute_reply.started":"2024-08-26T16:29:09.449373Z","shell.execute_reply":"2024-08-26T16:29:09.454781Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 xformers==0.0.21","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:09.460185Z","iopub.execute_input":"2024-08-26T16:29:09.460489Z","iopub.status.idle":"2024-08-26T16:29:09.465236Z","shell.execute_reply.started":"2024-08-26T16:29:09.460452Z","shell.execute_reply":"2024-08-26T16:29:09.464393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Data Collection and Aggregation**","metadata":{"id":"cTq1OnD0dGTn"}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nroot_path = \"/kaggle/input/nsl-videos\"\nfolder_list = os.listdir(root_path)\nlabel_list = [path for path in folder_list if not path.endswith((\".csv\"))]\n\ntotal_df = pd.read_csv('/kaggle/input/trainfilepath/train.csv')\n\ntotal_df.reset_index(drop = True, inplace = True)\ntotal_df['label'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"xCaVkbTVX4qI","outputId":"a36b6690-fcc7-4369-be5b-e519e3759469","execution":{"iopub.status.busy":"2024-08-26T16:29:09.472876Z","iopub.execute_input":"2024-08-26T16:29:09.473174Z","iopub.status.idle":"2024-08-26T16:29:09.849895Z","shell.execute_reply.started":"2024-08-26T16:29:09.473142Z","shell.execute_reply":"2024-08-26T16:29:09.848736Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"label\nम संग अण्डा छैन ।                  36\nम तिम्रो पैसा खान्छु ।             36\nतिमी म लाई मनपर्छ ।                34\nतिमी हरु मेरो साथी हो ।            34\nतिम्रो काम हरु म लाई छैन ।         34\nम लाई भक्तपुर मनपर्छ ।             34\nम संग मेरो साथी छ ।                34\nभक्तपुर मा धेरै काम छ ।            34\nम घर मा धेरै काम गर्छु ।           33\nतिमी संग अण्डा छैन ।               32\nम अण्डा खान्छु ।                   32\nमेरो साथी लाई अण्डा मनपर्छ ।       32\nतिम्रो काम छैन पैसा छैन ।          32\nतिम्रो काम धेरै छ ।                32\nमेरो धेरै साथी हरु छन् ।           30\nम लाई अण्डा मनपर्छ ।               28\nम संग धेरै पैसा छैन ।              26\nमेरो घर भक्तपुर मा छ ।             18\nम भक्तपुर मा काम गर्छु ।           18\nमेरो साथी धेरै भक्तपुर मा छन् ।    16\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Data Splitting**","metadata":{"id":"8WeH5kRLX4qJ"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef correct_file_path(file_name: str, root_path: str):\n    return os.path.join(root_path, file_name)\n\ndef preprocess_meta_df(df, root_path, label2id):\n    df.rename(columns={\"video_name\": \"video_path\"}, inplace=True)\n    df['video_path'] = df['video_path'].apply(lambda x: correct_file_path(x, root_path))\n#     df['label'] = df['label'].apply(lambda x: label2id[x])\n    df['label'] = df['label']\n    \n    return df\n\ntrain_meta_df, test_meta_df = train_test_split(total_df, test_size=0.2, stratify=total_df['label'], random_state=42)\n\nlabel_list = list(set(train_meta_df['label']))\nclass_labels = sorted(label_list)\nlabel2id = {label: i for i, label in enumerate(class_labels)}\nid2label = {i: label for label, i in label2id.items()}\n\nprint(f\"Unique classes: {list(label2id.keys())}.\")\n\ntrain_meta_df = preprocess_meta_df(train_meta_df, root_path, label2id)\ntest_meta_df = preprocess_meta_df(test_meta_df, root_path, label2id)\n\nprint(\"Splitted data:\", len(train_meta_df), len(test_meta_df))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_m-QX6C8X4qJ","outputId":"3221ede6-6ecf-43fe-b466-53d8bff199cb","execution":{"iopub.status.busy":"2024-08-26T16:29:09.851946Z","iopub.execute_input":"2024-08-26T16:29:09.852687Z","iopub.status.idle":"2024-08-26T16:29:10.334953Z","shell.execute_reply.started":"2024-08-26T16:29:09.852638Z","shell.execute_reply":"2024-08-26T16:29:10.333903Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Unique classes: ['तिमी म लाई मनपर्छ ।', 'तिमी संग अण्डा छैन ।', 'तिमी हरु मेरो साथी हो ।', 'तिम्रो काम छैन पैसा छैन ।', 'तिम्रो काम धेरै छ ।', 'तिम्रो काम हरु म लाई छैन ।', 'भक्तपुर मा धेरै काम छ ।', 'म अण्डा खान्छु ।', 'म घर मा धेरै काम गर्छु ।', 'म तिम्रो पैसा खान्छु ।', 'म भक्तपुर मा काम गर्छु ।', 'म लाई अण्डा मनपर्छ ।', 'म लाई भक्तपुर मनपर्छ ।', 'म संग अण्डा छैन ।', 'म संग धेरै पैसा छैन ।', 'म संग मेरो साथी छ ।', 'मेरो घर भक्तपुर मा छ ।', 'मेरो धेरै साथी हरु छन् ।', 'मेरो साथी धेरै भक्तपुर मा छन् ।', 'मेरो साथी लाई अण्डा मनपर्छ ।'].\nSplitted data: 484 121\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  **Model Selection and Design**","metadata":{"id":"0OSo11N6X4qK"}},{"cell_type":"markdown","source":"## Preparing Configuration for the final Model","metadata":{}},{"cell_type":"code","source":"import torch\ndef get_config():\n    return {\n        \"batch_size\": 1,\n        \"num_epochs\": 2,\n        \"lr\": 10**-4,\n        \"seq_len\": 1470,\n        \"d_model\": 768,\n        \"lang_tgt\": \"ne\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": \"latest\",\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"experiment_name\": \"/content/drive/MyDrive/English2Nepali/runs\"\n        # \"experiment_name\": \"/content/drive/MyDrive/translation/runs\"\n    }\n\nconfig=get_config()\ndevice=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:10.336142Z","iopub.execute_input":"2024-08-26T16:29:10.336675Z","iopub.status.idle":"2024-08-26T16:29:11.958985Z","shell.execute_reply.started":"2024-08-26T16:29:10.336635Z","shell.execute_reply":"2024-08-26T16:29:11.957945Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pre-trained Encoder Model (Video Vision Transformer)","metadata":{}},{"cell_type":"code","source":"import pytorchvideo.data\nfrom torch.utils.data import Dataset\nfrom transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    Normalize,\n    RandomShortSideScale,\n    RemoveKey,\n    ShortSideScale,\n    UniformTemporalSubsample,\n)\n\nfrom torchvision.transforms import (\n    Compose,\n    Lambda,\n    RandomCrop,\n    RandomHorizontalFlip,\n    Resize,\n)\n\nmodel_checkpoint = \"google/vivit-b-16x2-kinetics400\"\nimage_processor = VivitImageProcessor.from_pretrained(model_checkpoint)\n\n# model = VivitForVideoClassification.from_pretrained(model_checkpoint)\nvivit_model=VivitModel.from_pretrained(model_checkpoint)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsdszFloX4qK","outputId":"8de48368-2efd-4503-8255-2fcae2b354a9","execution":{"iopub.status.busy":"2024-08-26T16:29:11.962002Z","iopub.execute_input":"2024-08-26T16:29:11.962868Z","iopub.status.idle":"2024-08-26T16:29:17.125069Z","shell.execute_reply.started":"2024-08-26T16:29:11.962826Z","shell.execute_reply":"2024-08-26T16:29:17.124006Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"vivit_model","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:17.126600Z","iopub.execute_input":"2024-08-26T16:29:17.127792Z","iopub.status.idle":"2024-08-26T16:29:17.135748Z","shell.execute_reply.started":"2024-08-26T16:29:17.127744Z","shell.execute_reply":"2024-08-26T16:29:17.134792Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"VivitModel(\n  (embeddings): VivitEmbeddings(\n    (patch_embeddings): VivitTubeletEmbeddings(\n      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): VivitEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x VivitLayer(\n        (attention): VivitAttention(\n          (attention): VivitSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): VivitSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): VivitIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n          (intermediate_act_fn): FastGELUActivation()\n        )\n        (output): VivitOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      )\n    )\n  )\n  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (pooler): VivitPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Don't Need the Final Pooler Layer","metadata":{}},{"cell_type":"code","source":"#Replace pooler layer with the identity function, it just returns what it gets\n\nclass Identity(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return x\n\n# model.classifier=Identity()\n# model.classifier=torch.nn.Linear(768,5)\n\n# patch size of 32*32\nvivit_model.config.tubelet_size=[2,32,32]\n\n# 6 encoder block stacks\nvivit_model.config.num_hidden_layers=12\n\n# dropout set to 0.1\nvivit_model.config.hidden_dropout_prob=0.3\n\n# number of frames extracting from each video\nvivit_model.config.num_frames=60\n\nvivit_model=VivitModel(vivit_model.config)\n\nvivit_model.pooler=Identity()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:17.137057Z","iopub.execute_input":"2024-08-26T16:29:17.137455Z","iopub.status.idle":"2024-08-26T16:29:18.555396Z","shell.execute_reply.started":"2024-08-26T16:29:17.137393Z","shell.execute_reply":"2024-08-26T16:29:18.554311Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vivit_model","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.556710Z","iopub.execute_input":"2024-08-26T16:29:18.557035Z","iopub.status.idle":"2024-08-26T16:29:18.565276Z","shell.execute_reply.started":"2024-08-26T16:29:18.557000Z","shell.execute_reply":"2024-08-26T16:29:18.564335Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"VivitModel(\n  (embeddings): VivitEmbeddings(\n    (patch_embeddings): VivitTubeletEmbeddings(\n      (projection): Conv3d(3, 768, kernel_size=(2, 32, 32), stride=(2, 32, 32))\n    )\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n  (encoder): VivitEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x VivitLayer(\n        (attention): VivitAttention(\n          (attention): VivitSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): VivitSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.3, inplace=False)\n          )\n        )\n        (intermediate): VivitIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (dropout): Dropout(p=0.3, inplace=False)\n          (intermediate_act_fn): FastGELUActivation()\n        )\n        (output): VivitOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.3, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      )\n    )\n  )\n  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (pooler): Identity()\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Custom Decoder Model","metadata":{}},{"cell_type":"code","source":"import math\n\nclass InputEmbeddings(torch.nn.Module):\n\n    def __init__(self, d_model: int=768, vocab_size: int=27) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = torch.nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        # (batch, seq_len) --> (batch, seq_len, d_model)\n        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n        return self.embedding(x) * math.sqrt(self.d_model)\n\n\nclass PositionalEncoding(torch.nn.Module):\n\n    def __init__(self, d_model: int=768, seq_len: int=1225, dropout: float=0.1) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = torch.nn.Dropout(dropout)\n        # Create a matrix of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        # Create a vector of shape (seq_len)\n        position = torch.arange(\n            0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n        # Create a vector of shape (d_model)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n        ) * (-math.log(10000.0) / d_model))  # (d_model / 2)\n        # Apply sine to even indices\n        # sin(position * (10000 ** (2i / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        # Apply cosine to odd indices\n        # cos(position * (10000 ** (2i / d_model))\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # Add a batch dimension to the positional encoding\n        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n        # Register the positional encoding as a buffer\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # (batch, seq_len, d_model)\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)\n    \n    \n    \nclass MultiHeadAttentionBlock(torch.nn.Module):\n\n    def __init__(self, d_model: int=768, h: int=8, dropout: float=0.1) -> None:\n        super().__init__()\n        self.d_model = d_model  # Embedding vector size\n        self.h = h  # Number of heads\n        # Make sure d_model is divisible by h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n\n        self.d_k = d_model // h  # Dimension of vector seen by each head\n        self.w_q = torch.nn.Linear(d_model, d_model, bias=False)  # Wq\n        self.w_k = torch.nn.Linear(d_model, d_model, bias=False)  # Wk\n        self.w_v = torch.nn.Linear(d_model, d_model, bias=False)  # Wv\n        self.w_o = torch.nn.Linear(d_model, d_model, bias=False)  # Wo\n        self.dropout = torch.nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: torch.nn.Dropout):\n        d_k = query.shape[-1]\n        # Just apply the formula from the paper\n        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            # Write a very low value (indicating -inf) to the positions where mask == 0\n            attention_scores.masked_fill_(mask == 0, -1e9)\n        # (batch, h, seq_len, seq_len) # Apply softmax\n        attention_scores = attention_scores.softmax(dim=-1)\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n        # return attention scores which can be used for visualization\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        query = self.w_q(q)\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        key = self.w_k(k)\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        value = self.w_v(v)\n\n        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n        query = query.view(\n            query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1],\n                       self.h, self.d_k).transpose(1, 2)\n        value = value.view(\n            value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n        # Calculate attention\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n            query, key, value, mask, self.dropout)\n\n        # Combine all the heads together\n        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n        x = x.transpose(1, 2).contiguous().view(\n            x.shape[0], -1, self.h * self.d_k)\n\n        # Multiply by Wo\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n        return self.w_o(x)\n    \n\nclass LayerNormalization(torch.nn.Module):\n\n    def __init__(self, features: int, eps: float = 10**-6) -> None:\n        super().__init__()\n        self.eps = eps\n        # alpha is a learnable parameter\n        self.alpha = torch.nn.Parameter(torch.ones(features))\n        # bias is a learnable parameter\n        self.bias = torch.nn.Parameter(torch.zeros(features))\n\n    def forward(self, x):\n        # x: (batch, seq_len, hidden_size)\n        # Keep the dimension for broadcasting\n        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n        # Keep the dimension for broadcasting\n        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n        # eps is to prevent dividing by zero or when std is very small\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n\n\nclass FeedForwardBlock(torch.nn.Module):\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.linear_1 = torch.nn.Linear(d_model, d_ff)  # w1 and b1\n        self.dropout = torch.nn.Dropout(dropout)\n        self.linear_2 = torch.nn.Linear(d_ff, d_model)  # w2 and b2\n\n    def forward(self, x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n\n\nclass ResidualConnection(torch.nn.Module):\n\n    def __init__(self, features: int, dropout: float) -> None:\n        super().__init__()\n        self.dropout = torch.nn.Dropout(dropout)\n        self.norm = LayerNormalization(features)\n\n    # many transformer implmentation also do like this--> normalize the input + positional embedding, then apply mhsa and add skip connection.\n    # here sublayer is MHSA\n    def forward(self, x, sublayer):\n      return x + self.dropout(self.norm(sublayer(x)))\n\n\n\nclass DecoderBlock(torch.nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = torch.nn.ModuleList(\n            [ResidualConnection(features, dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](\n            x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(\n            x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x\n\n\nclass Decoder(torch.nn.Module):\n\n    def __init__(self, features: int, layers: torch.nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)\n\n\nclass ProjectionLayer(torch.nn.Module):\n\n    def __init__(self, d_model, vocab_size) -> None:\n        super().__init__()\n        self.proj = torch.nn.Linear(d_model, vocab_size)\n\n    def forward(self, x) -> None:\n        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n        return self.proj(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.566656Z","iopub.execute_input":"2024-08-26T16:29:18.566960Z","iopub.status.idle":"2024-08-26T16:29:18.605007Z","shell.execute_reply.started":"2024-08-26T16:29:18.566928Z","shell.execute_reply":"2024-08-26T16:29:18.604099Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Building Video2Text Transformer Architecture","metadata":{}},{"cell_type":"code","source":"class Video2Text(torch.nn.Module):\n\n    def __init__(self, encoder, decoder: Decoder, tgt_embed: InputEmbeddings, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n        self.video_encoder = encoder\n        self.decoder = decoder\n        self.tgt_embed = tgt_embed\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n\n    def encode(self,src_video):\n    # (batch,num_frames, num_channels, height, width)\n        if src_video != None:\n            perumuted_sample_test_video = src_video.permute(0,2, 1, 3, 4)\n\n            inputs = {\n                \"pixel_values\": perumuted_sample_test_video,\n            }\n            # forward pass\n            outputs = self.video_encoder(**inputs)\n            \n#           first token in the sequence is the class token. so, we dont need that. (batchsize, seq_len, embedding)\n            return outputs.last_hidden_state[:,1:,:]\n        else:\n            return None\n\n    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n        # (batch, seq_len, d_model)\n        tgt = self.tgt_embed(tgt)\n        tgt = self.tgt_pos(tgt)\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n    def project(self, x):\n        # (batch, seq_len, vocab_size)\n        return self.projection_layer(x)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.606164Z","iopub.execute_input":"2024-08-26T16:29:18.606543Z","iopub.status.idle":"2024-08-26T16:29:18.619586Z","shell.execute_reply.started":"2024-08-26T16:29:18.606498Z","shell.execute_reply":"2024-08-26T16:29:18.618663Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def build_transformer(encoder_model,tgt_vocab_size: int, tgt_seq_len: int, d_model: int = 768, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Video2Text:\n    \n    # Create the embedding layers\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n\n    # Create the positional encoding layers\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n\n    # Create the decoder blocks\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttentionBlock(\n            d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(\n            d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(d_model, decoder_self_attention_block,\n                                     decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n\n    # Create the encoder and decoder\n    video_encoder = encoder_model\n    decoder = Decoder(d_model, torch.nn.ModuleList(decoder_blocks))\n\n    # Create the projection layer\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n\n    # Create the transformer\n    transformer = Video2Text(\n        encoder=video_encoder,decoder=decoder,tgt_embed=tgt_embed, tgt_pos=tgt_pos, projection_layer=projection_layer)\n\n    # Initialize the parameters\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            torch.nn.init.xavier_uniform_(p)\n\n\n    return transformer","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.622946Z","iopub.execute_input":"2024-08-26T16:29:18.623946Z","iopub.status.idle":"2024-08-26T16:29:18.633675Z","shell.execute_reply.started":"2024-08-26T16:29:18.623908Z","shell.execute_reply":"2024-08-26T16:29:18.632676Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_model(config,enc_model,vocab_tgt_len):\n    v2t_model = build_transformer(encoder_model=enc_model,tgt_vocab_size=vocab_tgt_len,\n                              tgt_seq_len=config['seq_len'], d_model=config['d_model'])\n    return v2t_model","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.634885Z","iopub.execute_input":"2024-08-26T16:29:18.635231Z","iopub.status.idle":"2024-08-26T16:29:18.646193Z","shell.execute_reply.started":"2024-08-26T16:29:18.635173Z","shell.execute_reply":"2024-08-26T16:29:18.645376Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"> # **Apply Necessary Transform and Prepare Dataset**","metadata":{"id":"aiN2WpiMX4qK"}},{"cell_type":"code","source":"class CustomVideoDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        video_path = row['video_path']\n        label = row['label']\n        return video_path, label\n\nmean = image_processor.image_mean\nstd = image_processor.image_std\n\nif \"shortest_edge\" in image_processor.size:\n    height = width = image_processor.size[\"shortest_edge\"]\nelse:\n    height = image_processor.size[\"height\"]\n    width = image_processor.size[\"width\"]\n\nresize_to = (vivit_model.config.image_size, vivit_model.config.image_size)\n\n# num_frames_to_sample = model.config.num_frames\nnum_frames_to_sample = 60\nclip_duration = 10\n\ntrain_transform = Compose(\n    [\n        ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose(\n                [\n                    UniformTemporalSubsample(num_frames_to_sample),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n#                     RandomShortSideScale(min_size=256, max_size=320),\n                    Resize(resize_to),\n#                     RandomHorizontalFlip(p=0.5),\n                ]\n            ),\n        ),\n    ]\n)\n\nval_transform = Compose(\n    [\n        ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose(\n                [\n                    UniformTemporalSubsample(num_frames_to_sample),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    Resize(resize_to),\n                ]\n            ),\n        ),\n    ]\n)\n\ntrain_custom_dataset = CustomVideoDataset(train_meta_df)\ntrain_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in train_custom_dataset]\n\ntest_custom_dataset = CustomVideoDataset(test_meta_df)\ntest_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in test_custom_dataset]","metadata":{"id":"NGEtddR_X4qK","execution":{"iopub.status.busy":"2024-08-26T16:29:18.647526Z","iopub.execute_input":"2024-08-26T16:29:18.648186Z","iopub.status.idle":"2024-08-26T16:29:18.691623Z","shell.execute_reply.started":"2024-08-26T16:29:18.648143Z","shell.execute_reply":"2024-08-26T16:29:18.690681Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import imageio\nimport numpy as np\nfrom IPython.display import Image\n\ntrain_dataset = pytorchvideo.data.LabeledVideoDataset(\n    labeled_video_paths =train_labeled_video_paths,\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n    decode_audio=False,\n    transform=train_transform,\n)\n\ntest_dataset = pytorchvideo.data.LabeledVideoDataset(\n    labeled_video_paths =test_labeled_video_paths,\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n    decode_audio=False,\n    transform=val_transform,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"Moy-JNAIX4qL","outputId":"db052e2b-041a-4b79-9ca2-25df1f7ebe5a","execution":{"iopub.status.busy":"2024-08-26T16:29:18.692999Z","iopub.execute_input":"2024-08-26T16:29:18.693717Z","iopub.status.idle":"2024-08-26T16:29:18.710900Z","shell.execute_reply.started":"2024-08-26T16:29:18.693667Z","shell.execute_reply":"2024-08-26T16:29:18.709854Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class CustomVideoDataset2(Dataset):\n\n    def __init__(self, vdataset, tokenizer_tgt, tgt_lang, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n\n        self.vdataset = vdataset\n        self.tokenizer_tgt = tokenizer_tgt\n        self.tgt_lang = tgt_lang\n\n        self.sos_token = torch.tensor(\n            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor(\n            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor(\n            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    def __len__(self):\n#         return len(self.dataframe)\n        return self.vdataset.num_videos\n\n    def __getitem__(self, idx):\n        #new code here\n        \n        video=next(iter(self.vdataset))['video']\n#         label=next(iter(self.vdataset))['label']\n        target_txt=next(iter(self.vdataset))['label']\n        \n\n        # Transform the output text into tokens\n        dec_input_tokens = self.tokenizer_tgt.encode(target_txt).ids\n\n\n         # We will only add <s> here, and </s> only on the label\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n\n#         # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n        if dec_num_padding_tokens < 0:\n            raise ValueError(\"Sentence is too long\")\n\n\n#          Add only <s> token\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                torch.tensor([self.pad_token] *\n                             dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n#          Add only </eos> token\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] *\n                             dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n         )\n\n#         # Double check the size of the tensors to make sure they are all seq_len long\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            \"video\":video,\n            \"label\":label,\n            \"decoder_input\":decoder_input,\n            \"decoder_mask\":(decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n            \"tgt_text\":target_txt\n            \n        }\n\ndef causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.711969Z","iopub.execute_input":"2024-08-26T16:29:18.712264Z","iopub.status.idle":"2024-08-26T16:29:18.726596Z","shell.execute_reply.started":"2024-08-26T16:29:18.712225Z","shell.execute_reply":"2024-08-26T16:29:18.725743Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Initializing the model","metadata":{}},{"cell_type":"code","source":"from tokenizers import Tokenizer\n\ntarget_tokenizer=Tokenizer.from_file(str('/kaggle/input/nepalitokenizer/tokenizer_sign_lang_ne.json'))\n\n# initialize the model\nv2t_model=get_model(config=config,enc_model=vivit_model,vocab_tgt_len=target_tokenizer.get_vocab_size())\n    \nv2t_model.to(device)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:18.727739Z","iopub.execute_input":"2024-08-26T16:29:18.728359Z","iopub.status.idle":"2024-08-26T16:29:20.521927Z","shell.execute_reply.started":"2024-08-26T16:29:18.728315Z","shell.execute_reply":"2024-08-26T16:29:20.521022Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Video2Text(\n  (video_encoder): VivitModel(\n    (embeddings): VivitEmbeddings(\n      (patch_embeddings): VivitTubeletEmbeddings(\n        (projection): Conv3d(3, 768, kernel_size=(2, 32, 32), stride=(2, 32, 32))\n      )\n      (dropout): Dropout(p=0.3, inplace=False)\n    )\n    (encoder): VivitEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VivitLayer(\n          (attention): VivitAttention(\n            (attention): VivitSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): VivitSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.3, inplace=False)\n            )\n          )\n          (intermediate): VivitIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.3, inplace=False)\n            (intermediate_act_fn): FastGELUActivation()\n          )\n          (output): VivitOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.3, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (pooler): Identity()\n  )\n  (decoder): Decoder(\n    (layers): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (self_attention_block): MultiHeadAttentionBlock(\n          (w_q): Linear(in_features=768, out_features=768, bias=False)\n          (w_k): Linear(in_features=768, out_features=768, bias=False)\n          (w_v): Linear(in_features=768, out_features=768, bias=False)\n          (w_o): Linear(in_features=768, out_features=768, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (cross_attention_block): MultiHeadAttentionBlock(\n          (w_q): Linear(in_features=768, out_features=768, bias=False)\n          (w_k): Linear(in_features=768, out_features=768, bias=False)\n          (w_v): Linear(in_features=768, out_features=768, bias=False)\n          (w_o): Linear(in_features=768, out_features=768, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward_block): FeedForwardBlock(\n          (linear_1): Linear(in_features=768, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear_2): Linear(in_features=2048, out_features=768, bias=True)\n        )\n        (residual_connections): ModuleList(\n          (0-2): 3 x ResidualConnection(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (norm): LayerNormalization()\n          )\n        )\n      )\n    )\n    (norm): LayerNormalization()\n  )\n  (tgt_embed): InputEmbeddings(\n    (embedding): Embedding(27, 768)\n  )\n  (tgt_pos): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (projection_layer): ProjectionLayer(\n    (proj): Linear(in_features=768, out_features=27, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Training Without Huggingface Trainer**","metadata":{}},{"cell_type":"markdown","source":"## Prepare and Test Dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader, random_split\n\nnew_train_dataset=CustomVideoDataset2(train_dataset,target_tokenizer,config['lang_tgt'],config['seq_len'])\nnew_val_dataset=CustomVideoDataset2(test_dataset,target_tokenizer,config['lang_tgt'],config['seq_len'])\n\ntrain_dataloader = DataLoader(new_train_dataset, batch_size=1,shuffle=True)\nval_dataloader = DataLoader(new_val_dataset, batch_size=1,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.523129Z","iopub.execute_input":"2024-08-26T16:29:20.523512Z","iopub.status.idle":"2024-08-26T16:29:20.529841Z","shell.execute_reply.started":"2024-08-26T16:29:20.523468Z","shell.execute_reply":"2024-08-26T16:29:20.528916Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# i=0\n# for data in train_dataloader:\n#     i+=1\n#     if i>3:\n#         break\n#     print(data['tgt_text'],data['label'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.530942Z","iopub.execute_input":"2024-08-26T16:29:20.531201Z","iopub.status.idle":"2024-08-26T16:29:20.541251Z","shell.execute_reply.started":"2024-08-26T16:29:20.531172Z","shell.execute_reply":"2024-08-26T16:29:20.540407Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"def greedy_decode(model, src_video, source_mask, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    # Precompute the encoder output and reuse it for every step\n    encoder_output = model.encode(src_video=src_video)\n#     encoder_output = (torch.randint(2,7,(1,784,768))).type_as(encoder_output).to(device)\n    \n#     print(f'encoder_output: {encoder_output[:,392:400,:20]}')\n    # Initialize the decoder input with the sos token\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(src_video.type(torch.LongTensor)).to(device)\n    \n#     print(f\"decoder input: {decoder_input,decoder_input.shape}\")\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        # build mask for target\n        decoder_mask = causal_mask(decoder_input.size(\n            1)).type_as(src_video.type(torch.LongTensor)).to(device)\n        \n        \n#         print(f'decoder mask: {decoder_mask,decoder_mask.shape}')\n\n        # calculate output\n        out = model.decode(encoder_output=encoder_output, src_mask=None,tgt=decoder_input, tgt_mask=decoder_mask)\n\n        # get next token\n        prob = model.project(out[:, -1])\n        \n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat(\n            [decoder_input, torch.empty(1, 1).type_as(src_video.type(torch.LongTensor)).fill_(next_word.item()).to(device)], dim=1\n        )\n        \n#         print(f'next_word: {next_word}')\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)\n\n\n# model, src_video, source_mask, tokenizer_tgt, max_len, device\ndef beam_search_decode(model, beam_size, src_video, source_mask, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    # Precompute the encoder output and reuse it for every step\n    encoder_output = model.encode(src_video=src_video)\n    # Initialize the decoder input with the sos token\n    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(src_video.type(torch.LongTensor)).to(device)\n\n    # Create a candidate list\n    candidates = [(decoder_initial_input, 1)]\n\n    while True:\n\n        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n        if any([cand.size(1) == max_len for cand, _ in candidates]):\n            break\n\n        # Create a new list of candidates\n        new_candidates = []\n\n        for candidate, score in candidates:\n\n            # Do not expand candidates that have reached the eos token\n            if candidate[0][-1].item() == eos_idx:\n                continue\n\n            # Build the candidate's mask\n            candidate_mask = causal_mask(candidate.size(1)).type_as(src_video.type(torch.LongTensor)).to(device)\n            \n            # calculate output\n            out = model.decode(encoder_output=encoder_output, src_mask=None, tgt=candidate, tgt_mask=candidate_mask)\n            \n            # get next token probabilities\n            prob = model.project(out[:, -1])\n            \n            # get the top k candidates\n            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n            \n            for i in range(beam_size):\n                # for each of the top k candidates, get the token and its probability\n                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n                token_prob = topk_prob[0][i].item()\n                # create a new candidate by appending the token to the current candidate\n                new_candidate = torch.cat([candidate, token], dim=1)\n                # We sum the log probabilities because the probabilities are in log space\n                new_candidates.append((new_candidate, score + token_prob))\n\n        # Sort the new candidates by their score\n        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n        # Keep only the top k candidates\n        candidates = candidates[:beam_size]\n\n        # If all the candidates have reached the eos token, stop\n        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n            break\n\n    # Return the best candidate\n    return candidates[0][0].squeeze()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.542956Z","iopub.execute_input":"2024-08-26T16:29:20.543445Z","iopub.status.idle":"2024-08-26T16:29:20.562052Z","shell.execute_reply.started":"2024-08-26T16:29:20.543372Z","shell.execute_reply":"2024-08-26T16:29:20.561242Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import torchmetrics\n\ndef run_validation(model, validation_ds, tokenizer_tgt, max_len, device, print_msg, num_examples=2):\n    model.eval()\n    count = 0\n\n#     expected = []\n#     predicted = []\n\n    try:\n        # get the console window width\n        with os.popen('stty size', 'r') as console:\n            _, console_width = console.read().split()\n            console_width = int(console_width)\n    except:\n        # If we can't get the console width, use 80 as default\n        console_width = 80\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"video\"].to(device)  # (b, seq_len)\n\n            # check that the batch size is 1\n            assert encoder_input.size(\n                0) == 1, \"Batch size must be 1 for validation\"\n\n            model_out_greedy = greedy_decode(\n                model, encoder_input, None, tokenizer_tgt, max_len, device)\n            model_out_beam = beam_search_decode(model, 3, encoder_input, None, tokenizer_tgt, max_len, device)\n\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text_beam = tokenizer_tgt.decode(model_out_beam.detach().cpu().numpy())\n            model_out_text_greedy = tokenizer_tgt.decode(model_out_greedy.detach().cpu().numpy())\n\n#             source_texts.append(source_text)\n#             expected.append(target_text)\n#             predicted.append(model_out_text_greedy)\n\n            # Print the source, target and model output\n            if count <4:\n                print_msg(f\"{f'TARGET: ':>20}{target_text}\")\n                print_msg(f\"{f'PREDICTED GREEDY: ':>20}{model_out_text_greedy}\")\n                print_msg(f\"{f'PREDICTED BEAM: ':>20}{model_out_text_beam}\")\n\n            # print(count)\n            if count == num_examples:\n                print_msg('-'*console_width)\n                break\n        # Compute the word error rate   \n#         metric = torchmetrics.WordErrorRate()\n#         v_wer = metric(predicted, expected)\n#         print(f\"Word Error Rate:{v_wer}\")\n#         return v_wer\n\n        # Compute the BLEU metric\n#         metric = torchmetrics.BLEUScore()\n#         bleu = metric(predicted, expected)\n#         writer.add_scalar('validation BLEU', bleu, global_step)\n#         writer.flush()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.563413Z","iopub.execute_input":"2024-08-26T16:29:20.563757Z","iopub.status.idle":"2024-08-26T16:29:20.732826Z","shell.execute_reply.started":"2024-08-26T16:29:20.563723Z","shell.execute_reply":"2024-08-26T16:29:20.731835Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the saved model if notebook restarted","metadata":{}},{"cell_type":"code","source":"# saved_model=torch.load(\"/kaggle/working/99_mtrain.pt\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.733987Z","iopub.execute_input":"2024-08-26T16:29:20.734330Z","iopub.status.idle":"2024-08-26T16:29:20.738881Z","shell.execute_reply.started":"2024-08-26T16:29:20.734297Z","shell.execute_reply":"2024-08-26T16:29:20.737928Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# v2t_model=saved_model['model'].to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.739990Z","iopub.execute_input":"2024-08-26T16:29:20.740274Z","iopub.status.idle":"2024-08-26T16:29:20.748705Z","shell.execute_reply.started":"2024-08-26T16:29:20.740242Z","shell.execute_reply":"2024-08-26T16:29:20.747635Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# saved_model['epoch']","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.750105Z","iopub.execute_input":"2024-08-26T16:29:20.750508Z","iopub.status.idle":"2024-08-26T16:29:20.758112Z","shell.execute_reply.started":"2024-08-26T16:29:20.750456Z","shell.execute_reply":"2024-08-26T16:29:20.757315Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# plt.plot(saved_model['train_loss'])\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:29:20.759209Z","iopub.execute_input":"2024-08-26T16:29:20.759537Z","iopub.status.idle":"2024-08-26T16:29:20.768071Z","shell.execute_reply.started":"2024-08-26T16:29:20.759506Z","shell.execute_reply":"2024-08-26T16:29:20.767293Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\noptimizer = torch.optim.Adam(v2t_model.parameters(), lr=10**-5, eps=1e-9)\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=target_tokenizer.token_to_id(\n        '[PAD]'),label_smoothing=0.1).to(device)\n\n\n# loss_list_train=saved_model['train_loss']\nloss_list_train=[]\nloss_list_val=[]\nwer_list_train=[]\nwer_list_val=[]\n\n\n# for epoch in range(saved_model['epoch']+1,200):\nfor epoch in range(100):\n    torch._C._cuda_emptyCache()\n    v2t_model.train()\n    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n    \n    #accumulate LOSS and WER\n    acc_loss=0\n    acc_wer=0\n    for batch in batch_iterator:\n        \n                src_video=batch['video'].to(device)\n                decoder_input=batch['decoder_input'].to(device)\n                decoder_mask=batch['decoder_mask'].to(device)\n                \n                 # Run the tensors through the encoder, decoder and the projection layer\n                encoder_output = v2t_model.encode(src_video=src_video)  # (B, seq_len, d_model)\n                decoder_output = v2t_model.decode(encoder_output=encoder_output, src_mask=None,tgt=decoder_input, tgt_mask=decoder_mask) # (B, seq_len, d_model)\n                 # (B, seq_len, vocab_size)\n                proj_output = v2t_model.project(decoder_output)\n                \n                 # Compare the output with the label\n                label = batch['label'].to(device)  # (B, seq_len)\n\n                 # Compute the loss using a simple cross entropy\n                loss = loss_fn(proj_output.view(-1, target_tokenizer.get_vocab_size()), label.view(-1))\n                \n                 # accumulated loss for every batch in a single epoch\n                acc_loss+=loss.item()\n                \n        \n                # Backpropagate the loss\n                loss.backward()\n\n                # Update the weights\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                \n                #calculating training WER\n                pred_tokens = torch.argmax(proj_output, dim=-1)  # Get the predicted token indices\n                pred_sentences = target_tokenizer.decode(pred_tokens.detach().cpu().numpy()[0][:8], skip_special_tokens=True)\n                metric = torchmetrics.text.WordErrorRate()\n                t_WER=metric(pred_sentences, batch['tgt_text'])\n                acc_wer+=t_WER\n                \n    \n    \n    loss_list_train.append(np.round(acc_loss/len(train_dataloader),3))\n    wer_list_train.append(np.round(acc_wer.numpy()/len(train_dataloader),3))\n    \n    # predict sentences\n    run_validation(v2t_model,val_dataloader, target_tokenizer, 15 ,device,lambda msg: batch_iterator.write(msg))\n    \n\n    v2t_model.eval()\n    acc_loss=0\n    acc_wer=0\n    \n    with torch.no_grad():\n        for batchv in val_dataloader:\n\n            src_video=batchv['video'].to(device)\n            decoder_input=batchv['decoder_input'].to(device)\n            decoder_mask=batchv['decoder_mask'].to(device)\n\n            # Run the tensors through the encoder, decoder and the projection layer\n            encoder_output = v2t_model.encode(src_video=src_video)  # (B, seq_len, d_model)\n            decoder_output = v2t_model.decode(encoder_output=encoder_output, src_mask=None,tgt=decoder_input, tgt_mask=decoder_mask) # (B, seq_len, d_model)\n            # (B, seq_len, vocab_size)\n            proj_output = v2t_model.project(decoder_output)\n\n            # Compare the output with the label\n            label = batchv['label'].to(device)  # (B, seq_len)\n\n            # Compute the loss using a simple cross entropy\n            val_loss = loss_fn(proj_output.view(-1, target_tokenizer.get_vocab_size()), label.view(-1))\n\n            acc_loss+=val_loss.item()\n\n            #calculating validation WER\n            pred_tokens = torch.argmax(proj_output, dim=-1)  # Get the predicted token indices\n            pred_sentences = target_tokenizer.decode(pred_tokens.detach().cpu().numpy()[0][:8], skip_special_tokens=True)\n            metric = torchmetrics.text.WordErrorRate()\n            v_WER=metric(pred_sentences, batchv['tgt_text'])\n            acc_wer+=v_WER\n        \n    with torch.no_grad():   \n        loss_list_val.append(np.round(acc_loss/len(val_dataloader),3))\n        wer_list_val.append(np.round(acc_wer.numpy()/len(val_dataloader),3))\n    \n    if (epoch+1)%5==0:\n        \n        torch.save({\"model_state_dict\":v2t_model.state_dict(),\n                    \"optimizer_state_dict\":optimizer.state_dict(),\n                    \"train_loss\":loss_list_train,\n                    \"val_loss\":loss_list_val,\n                   \"t_wer\":wer_list_train,\n                    \"v_wer\":wer_list_val,\n                    \"epoch\":epoch+1,\n                    },\n                    f\"{epoch}_mtrain.pt\")\n        \n    \n    print(f\" Epoch: {epoch} | Training Loss: {loss_list_train[-1]}      Validation Loss: {loss_list_val[-1]}\\\n             Train WER: {wer_list_train[-1]}      Validation WER: {wer_list_val[-1]}\")\n    \n                ","metadata":{"execution":{"iopub.status.busy":"2024-08-26T16:40:58.990778Z","iopub.execute_input":"2024-08-26T16:40:58.991922Z","iopub.status.idle":"2024-08-26T23:31:57.856772Z","shell.execute_reply.started":"2024-08-26T16:40:58.991873Z","shell.execute_reply":"2024-08-26T23:31:57.855417Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 484/484 [07:57<00:00,  1.01it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: म संग मेरो साथी छ । अण्डा छैन । अण्डा छैन ।\n            TARGET: तिम्रो काम हरु म लाई छैन ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: म संग मेरो साथी छ । अण्डा छैन । अण्डा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 0 | Training Loss: 1.34      Validation Loss: 1.269             Train WER: 0.576      Validation WER: 0.468\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 484/484 [08:13<00:00,  1.02s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म लाई भक्तपुर मनपर्छ ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो ।\n            TARGET: तिमी संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो ।\n--------------------------------------------------------------------------------\n Epoch: 1 | Training Loss: 1.226      Validation Loss: 1.177             Train WER: 0.523      Validation WER: 0.519\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 484/484 [08:15<00:00,  1.02s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म लाई भक्तपुर मनपर्छ ।\n  PREDICTED GREEDY: म तिम्रो पैसा खान्छु ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । म लाई छैन पैसा छैन ।\n            TARGET: तिम्रो काम धेरै छ ।\n  PREDICTED GREEDY: म तिम्रो पैसा खान्छु ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । म लाई छैन पैसा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 2 | Training Loss: 1.181      Validation Loss: 1.16             Train WER: 0.507      Validation WER: 0.506\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 484/484 [08:14<00:00,  1.02s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो धेरै साथी हरु छन् ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो ।\n            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो ।\n--------------------------------------------------------------------------------\n Epoch: 3 | Training Loss: 1.15      Validation Loss: 1.123             Train WER: 0.488      Validation WER: 0.45\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 04: 100%|██████████| 484/484 [08:10<00:00,  1.01s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म भक्तपुर मा काम गर्छु ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो ।\n            TARGET: तिमी संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो ।\n--------------------------------------------------------------------------------\n Epoch: 4 | Training Loss: 1.14      Validation Loss: 1.121             Train WER: 0.5      Validation WER: 0.46\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 05: 100%|██████████| 484/484 [07:34<00:00,  1.06it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म संग धेरै पैसा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: म संग मेरो साथी छ । भक्तपुर मा छन् । अण्डा छैन ।\n            TARGET: तिमी संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: म संग मेरो साथी छ । भक्तपुर मा छन् । अण्डा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 5 | Training Loss: 1.132      Validation Loss: 1.137             Train WER: 0.494      Validation WER: 0.481\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 06: 100%|██████████| 484/484 [07:59<00:00,  1.01it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिमी संग अण्डा छैन ।\n  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन । हरु म लाई छैन\n            TARGET: म भक्तपुर मा काम गर्छु ।\n  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन । हरु म लाई छैन\n--------------------------------------------------------------------------------\n Epoch: 6 | Training Loss: 1.113      Validation Loss: 1.112             Train WER: 0.501      Validation WER: 0.449\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 07: 100%|██████████| 484/484 [08:07<00:00,  1.01s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: भक्तपुर मा धेरै काम छ ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: मेरो साथी लाई अण्डा मनपर्छ । भक्तपुर मा छन् । तिमी हरु छन् ।\n            TARGET: तिमी हरु मेरो साथी हो ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: मेरो साथी लाई अण्डा मनपर्छ । भक्तपुर मा छन् । तिमी हरु छन् ।\n--------------------------------------------------------------------------------\n Epoch: 7 | Training Loss: 1.103      Validation Loss: 1.1             Train WER: 0.493      Validation WER: 0.472\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 08: 100%|██████████| 484/484 [08:04<00:00,  1.00s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । हरु छन् । म लाई अण्डा\n            TARGET: म भक्तपुर मा काम गर्छु ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । हरु छन् । म लाई अण्डा\n--------------------------------------------------------------------------------\n Epoch: 8 | Training Loss: 1.104      Validation Loss: 1.093             Train WER: 0.493      Validation WER: 0.481\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 09: 100%|██████████| 484/484 [08:06<00:00,  1.00s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम धेरै छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । धेरै भक्तपुर मा छन् मा छन् ।\n            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । धेरै भक्तपुर मा छन् मा छन् ।\n--------------------------------------------------------------------------------\n Epoch: 9 | Training Loss: 1.095      Validation Loss: 1.098             Train WER: 0.483      Validation WER: 0.494\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 10: 100%|██████████| 484/484 [08:19<00:00,  1.03s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । संग मेरो साथी छ । छैन ।\n            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । संग मेरो साथी छ । छैन ।\n--------------------------------------------------------------------------------\n Epoch: 10 | Training Loss: 1.086      Validation Loss: 1.101             Train WER: 0.476      Validation WER: 0.483\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 11: 100%|██████████| 484/484 [08:21<00:00,  1.04s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो साथी धेरै भक्तपुर मा छन् ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । घर भक्तपुर मा छन् । मेरो साथी हरु\n            TARGET: म संग मेरो साथी छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । घर भक्तपुर मा छन् । मेरो साथी हरु\n--------------------------------------------------------------------------------\n Epoch: 11 | Training Loss: 1.086      Validation Loss: 1.095             Train WER: 0.483      Validation WER: 0.456\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 12: 100%|██████████| 484/484 [08:31<00:00,  1.06s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: भक्तपुर मा धेरै काम छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: म लाई अण्डा मनपर्छ । अण्डा मनपर्छ । म लाई अण्डा मनपर्छ ।\n            TARGET: तिमी हरु मेरो साथी हो ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: म लाई अण्डा मनपर्छ । अण्डा मनपर्छ । म लाई अण्डा मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 12 | Training Loss: 1.084      Validation Loss: 1.137             Train WER: 0.473      Validation WER: 0.472\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 13: 100%|██████████| 484/484 [08:23<00:00,  1.04s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम धेरै छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: म संग मेरो साथी छ । मेरो साथी छ । मेरो साथी छ ।\n            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: म संग मेरो साथी छ । मेरो साथी छ । मेरो साथी छ ।\n--------------------------------------------------------------------------------\n Epoch: 13 | Training Loss: 1.079      Validation Loss: 1.108             Train WER: 0.474      Validation WER: 0.496\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 14: 100%|██████████| 484/484 [08:28<00:00,  1.05s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम धेरै छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । म लाई छैन । पैसा छैन ।\n            TARGET: म भक्तपुर मा काम गर्छु ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । म लाई छैन । पैसा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 14 | Training Loss: 1.079      Validation Loss: 1.08             Train WER: 0.486      Validation WER: 0.519\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 15: 100%|██████████| 484/484 [08:00<00:00,  1.01it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म भक्तपुर मा काम गर्छु ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम धेरै छ ।\n            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम धेरै छ ।\n--------------------------------------------------------------------------------\n Epoch: 15 | Training Loss: 1.079      Validation Loss: 1.111             Train WER: 0.486      Validation WER: 0.472\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 16: 100%|██████████| 484/484 [07:53<00:00,  1.02it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिमी संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । लाई अण्डा मनपर्छ । छन् ।\n            TARGET: म संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । मा छन् मा छन् मा छन् ।\n--------------------------------------------------------------------------------\n Epoch: 16 | Training Loss: 1.072      Validation Loss: 1.077             Train WER: 0.47      Validation WER: 0.457\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 17: 100%|██████████| 484/484 [08:22<00:00,  1.04s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम छैन पैसा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् ।\n            TARGET: तिमी हरु मेरो साथी हो ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् ।\n--------------------------------------------------------------------------------\n Epoch: 17 | Training Loss: 1.065      Validation Loss: 1.066             Train WER: 0.466      Validation WER: 0.475\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 18: 100%|██████████| 484/484 [08:09<00:00,  1.01s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म अण्डा खान्छु ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । तिम्रो पैसा छैन । भक्तपुर मनपर्छ ।\n            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । तिम्रो पैसा छैन । भक्तपुर मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 18 | Training Loss: 1.063      Validation Loss: 1.095             Train WER: 0.469      Validation WER: 0.485\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 19: 100%|██████████| 484/484 [08:06<00:00,  1.01s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । लाई छैन । तिम्रो पैसा छैन ।\n            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । लाई छैन । तिम्रो पैसा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 19 | Training Loss: 1.065      Validation Loss: 1.072             Train WER: 0.488      Validation WER: 0.474\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 20: 100%|██████████| 484/484 [08:06<00:00,  1.00s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम छैन पैसा छैन ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । अण्डा मनपर्छ ।\n            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । अण्डा मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 20 | Training Loss: 1.07      Validation Loss: 1.07             Train WER: 0.482      Validation WER: 0.499\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 21: 100%|██████████| 484/484 [08:17<00:00,  1.03s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म अण्डा खान्छु ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन ।\n            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन ।\n--------------------------------------------------------------------------------\n Epoch: 21 | Training Loss: 1.064      Validation Loss: 1.061             Train WER: 0.493      Validation WER: 0.503\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 22: 100%|██████████| 484/484 [08:10<00:00,  1.01s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो धेरै साथी हरु छन् ।\n  PREDICTED GREEDY: म अण्डा खान्छु ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । छैन । धेरै छ । तिम्रो ।\n            TARGET: म संग धेरै पैसा छैन ।\n  PREDICTED GREEDY: म अण्डा खान्छु ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । छैन । धेरै छ । तिम्रो ।\n--------------------------------------------------------------------------------\n Epoch: 22 | Training Loss: 1.072      Validation Loss: 1.073             Train WER: 0.49      Validation WER: 0.44\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 23: 100%|██████████| 484/484 [08:28<00:00,  1.05s/it]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम छैन पैसा छैन ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । हरु म लाई छैन । संग अण्डा छैन\n            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । छैन । हरु म लाई छैन ।\n--------------------------------------------------------------------------------\n Epoch: 23 | Training Loss: 1.064      Validation Loss: 1.074             Train WER: 0.485      Validation WER: 0.527\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 24: 100%|██████████| 484/484 [07:57<00:00,  1.01it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम हरु म लाई छैन ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । साथी हरु छन् । साथी हरु छन्\n            TARGET: मेरो धेरै साथी हरु छन् ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । साथी हरु छन् । साथी हरु छन्\n--------------------------------------------------------------------------------\n Epoch: 24 | Training Loss: 1.059      Validation Loss: 1.068             Train WER: 0.458      Validation WER: 0.498\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 25: 100%|██████████| 484/484 [07:45<00:00,  1.04it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म संग अण्डा छैन ।\n  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n    PREDICTED BEAM: तिमी म लाई मनपर्छ ।\n            TARGET: भक्तपुर मा धेरै काम छ ।\n  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n    PREDICTED BEAM: तिमी म लाई मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 25 | Training Loss: 1.055      Validation Loss: 1.08             Train WER: 0.459      Validation WER: 0.466\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 26: 100%|██████████| 484/484 [07:44<00:00,  1.04it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । तिमी संग मेरो साथी हो ।\n            TARGET: मेरो धेरै साथी हरु छन् ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । तिमी संग मेरो साथी हो ।\n--------------------------------------------------------------------------------\n Epoch: 26 | Training Loss: 1.059      Validation Loss: 1.051             Train WER: 0.47      Validation WER: 0.529\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 27: 100%|██████████| 484/484 [07:23<00:00,  1.09it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम हरु म लाई छैन ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो । लाई मनपर्छ ।\n            TARGET: म संग अण्डा छैन ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिमी हरु मेरो साथी हो । लाई मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 27 | Training Loss: 1.058      Validation Loss: 1.07             Train WER: 0.475      Validation WER: 0.501\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 28: 100%|██████████| 484/484 [07:36<00:00,  1.06it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n    PREDICTED BEAM: मेरो साथी लाई अण्डा मनपर्छ । म लाई अण्डा मनपर्छ ।\n            TARGET: मेरो घर भक्तपुर मा छ ।\n  PREDICTED GREEDY: म लाई अण्डा मनपर्छ ।\n    PREDICTED BEAM: मेरो साथी लाई अण्डा मनपर्छ । म लाई अण्डा मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 28 | Training Loss: 1.06      Validation Loss: 1.104             Train WER: 0.482      Validation WER: 0.49\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 29: 100%|██████████| 484/484 [07:18<00:00,  1.10it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो घर भक्तपुर मा छ ।\n  PREDICTED GREEDY: म अण्डा खान्छु ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । अण्डा मनपर्छ ।\n            TARGET: तिम्रो काम छैन पैसा छैन ।\n  PREDICTED GREEDY: म अण्डा खान्छु ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । अण्डा मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 29 | Training Loss: 1.059      Validation Loss: 1.066             Train WER: 0.478      Validation WER: 0.479\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 30: 100%|██████████| 484/484 [07:24<00:00,  1.09it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म संग धेरै पैसा छैन ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । हरु म लाई छैन । हरु म\n            TARGET: भक्तपुर मा धेरै काम छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । हरु म लाई छैन । हरु म\n--------------------------------------------------------------------------------\n Epoch: 30 | Training Loss: 1.051      Validation Loss: 1.074             Train WER: 0.457      Validation WER: 0.477\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 31: 100%|██████████| 484/484 [07:24<00:00,  1.09it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिमी संग अण्डा छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: म संग मेरो साथी छ । म लाई अण्डा छैन ।\n            TARGET: म लाई भक्तपुर मनपर्छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: म संग मेरो साथी छ । म लाई अण्डा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 31 | Training Loss: 1.056      Validation Loss: 1.099             Train WER: 0.46      Validation WER: 0.493\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 32: 100%|██████████| 484/484 [07:22<00:00,  1.09it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम छैन पैसा छैन ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । अण्डा मनपर्छ छैन । छैन ।\n            TARGET: तिम्रो काम छैन पैसा छैन ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । अण्डा मनपर्छ छैन । छैन ।\n--------------------------------------------------------------------------------\n Epoch: 32 | Training Loss: 1.051      Validation Loss: 1.056             Train WER: 0.474      Validation WER: 0.49\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 33: 100%|██████████| 484/484 [07:19<00:00,  1.10it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो धेरै साथी हरु छन् ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । हरु छन् । हरु छन् ।\n            TARGET: तिम्रो काम हरु म लाई छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो साथी धेरै भक्तपुर मा छन् । हरु छन् । हरु छन् ।\n--------------------------------------------------------------------------------\n Epoch: 33 | Training Loss: 1.05      Validation Loss: 1.062             Train WER: 0.471      Validation WER: 0.49\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 34: 100%|██████████| 484/484 [07:17<00:00,  1.11it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । छैन । धेरै छ । धेरै छ ।\n            TARGET: म लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । छैन । धेरै छ । धेरै छ ।\n--------------------------------------------------------------------------------\n Epoch: 34 | Training Loss: 1.049      Validation Loss: 1.077             Train WER: 0.472      Validation WER: 0.495\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 35: 100%|██████████| 484/484 [07:45<00:00,  1.04it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म अण्डा खान्छु ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । अण्डा छैन । छैन । छैन ।\n            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिम्रो काम छैन पैसा छैन । धेरै छ । अण्डा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 35 | Training Loss: 1.054      Validation Loss: 1.052             Train WER: 0.477      Validation WER: 0.46\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 36: 100%|██████████| 484/484 [07:18<00:00,  1.10it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: म संग धेरै पैसा छैन । संग धेरै पैसा छैन । धेरै पैसा छैन\n            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म संग धेरै पैसा छैन ।\n    PREDICTED BEAM: म संग धेरै पैसा छैन । संग धेरै पैसा छैन । धेरै पैसा छैन\n--------------------------------------------------------------------------------\n Epoch: 36 | Training Loss: 1.051      Validation Loss: 1.069             Train WER: 0.469      Validation WER: 0.485\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 37: 100%|██████████| 484/484 [07:23<00:00,  1.09it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म संग धेरै पैसा छैन ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन लाई छैन लाई छैन ।\n            TARGET: मेरो धेरै साथी हरु छन् ।\n  PREDICTED GREEDY: म लाई भक्तपुर मनपर्छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । लाई छैन लाई छैन लाई छैन ।\n--------------------------------------------------------------------------------\n Epoch: 37 | Training Loss: 1.052      Validation Loss: 1.061             Train WER: 0.479      Validation WER: 0.485\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 38: 100%|██████████| 484/484 [07:28<00:00,  1.08it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम धेरै छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । अण्डा मनपर्छ ।\n            TARGET: म तिम्रो पैसा खान्छु ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । अण्डा मनपर्छ ।\n--------------------------------------------------------------------------------\n Epoch: 38 | Training Loss: 1.051      Validation Loss: 1.046             Train WER: 0.479      Validation WER: 0.465\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 39: 100%|██████████| 484/484 [07:50<00:00,  1.03it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: म संग धेरै पैसा छैन ।\n  PREDICTED GREEDY: म तिम्रो पैसा खान्छु ।\n    PREDICTED BEAM: म तिम्रो पैसा खान्छु ।\n            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म तिम्रो पैसा खान्छु ।\n    PREDICTED BEAM: म तिम्रो पैसा खान्छु ।\n--------------------------------------------------------------------------------\n Epoch: 39 | Training Loss: 1.055      Validation Loss: 1.062             Train WER: 0.477      Validation WER: 0.443\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 40: 100%|██████████| 484/484 [07:38<00:00,  1.05it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो साथी धेरै भक्तपुर मा छन् ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । अण्डा छैन ।\n            TARGET: म घर मा धेरै काम गर्छु ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: मेरो धेरै साथी हरु छन् । अण्डा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 40 | Training Loss: 1.048      Validation Loss: 1.047             Train WER: 0.457      Validation WER: 0.481\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 41: 100%|██████████| 484/484 [07:56<00:00,  1.01it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: मेरो साथी लाई अण्डा मनपर्छ ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिमी संग अण्डा छैन ।\n            TARGET: मेरो साथी धेरै भक्तपुर मा छन् ।\n  PREDICTED GREEDY: म संग अण्डा छैन ।\n    PREDICTED BEAM: तिमी संग अण्डा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 41 | Training Loss: 1.045      Validation Loss: 1.045             Train WER: 0.464      Validation WER: 0.468\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 42: 100%|██████████| 484/484 [07:57<00:00,  1.01it/s]\nstty: 'standard input': Inappropriate ioctl for device\n","output_type":"stream"},{"name":"stdout","text":"            TARGET: तिम्रो काम हरु म लाई छैन ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । तिम्रो पैसा छैन ।\n            TARGET: तिमी म लाई मनपर्छ ।\n  PREDICTED GREEDY: म संग मेरो साथी छ ।\n    PREDICTED BEAM: तिम्रो काम हरु म लाई छैन । तिम्रो पैसा छैन ।\n--------------------------------------------------------------------------------\n Epoch: 42 | Training Loss: 1.043      Validation Loss: 1.058             Train WER: 0.455      Validation WER: 0.513\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 43:  44%|████▍     | 212/484 [03:27<04:26,  1.02it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#calculating training WER\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:449\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    446\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_exp_avgs, device_grads, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[0;32m--> 449\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_addcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# TODO: use foreach_pow if/when foreach_pow is added\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mpow(beta1, step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# v2t_model=torch.load('/kaggle/input/save-model/1_e_train.pt',map_location=torch.device('cpu'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Running Tests**","metadata":{}},{"cell_type":"markdown","source":"## Plot Each Frames Extracted from the Video","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nvideo=next(iter(val_dataloader))\n\n# Create subplots\nfig, axs = plt.subplots(8, 8, figsize=(10, 10))\n\n# Plot images\nf=0\nfor i in range(8):\n    for j in range(8):\n        f+=1\n        if f<60:\n          im=video['video'].permute(0,2,3,4,1)[0,f,:,:,:]\n          axs[i, j].imshow(im)\n        axs[i, j].set_title(f'frame: {f+1}')\n        axs[i, j].axis('off')  # Hide axis\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video['tgt_text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Inference on Single Video**","metadata":{}},{"cell_type":"code","source":"saved_model=torch.load('/kaggle/working/199_mtrain.pt')\nv2t_model=saved_model['model'].to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_inference(model, video, tokenizer_tgt, max_len, device):\n    model.eval()\n\n    source_texts = []\n    expected = []\n    predicted = []\n\n    with torch.no_grad():\n\n        encoder_input = video['video'][0].unsqueeze(0).to(device)  # (b, seq_len)\n\n        # check that the batch size is 1\n        assert encoder_input.size(\n            0) == 1, \"Batch size must be 1 for validation\"\n\n        model_out = greedy_decode(\n            model, encoder_input, None, tokenizer_tgt, max_len, device)\n\n        target_text = video[\"tgt_text\"][0]\n        model_out_text = tokenizer_tgt.decode(\n            model_out.detach().cpu().numpy())\n\n#             source_texts.append(source_text)\n        expected.append(target_text)\n        predicted.append(model_out_text)\n\n        # Print the source, target and model output\n        print('-----------------------------')\n        print(f\"TARGET: {target_text}\")\n        print(f\"PREDICTED: {model_out_text}\")\n\n        # Compute the word error rate   \n        metric = torchmetrics.WordErrorRate()\n        wer = metric(predicted, expected)\n        print(f\"Word Error Rate:{wer}\")","metadata":{"id":"A1qsy-kql_AT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video=next(iter(train_dataloader))\nrun_inference(model=v2t_model, video=video, tokenizer_tgt=target_tokenizer, max_len=15, device=device)","metadata":{"id":"z86pUo8gmys6","trusted":true},"execution_count":null,"outputs":[]}]}