{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dr3pNcYVdEFv","outputId":"43ec28c8-2f58-4c3b-c90d-0e26b9ffdd0d","trusted":true},"outputs":[],"source":["# !pip install -q pytorchvideo evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7q8Jg3l-ewNt","outputId":"d310913a-3855-4c12-b21c-3f2845d63ef7","trusted":true},"outputs":[],"source":["# pip install -q pyarrow==14.0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Rw1LiX8CB7i","trusted":true},"outputs":[],"source":["# pip install -q transformers --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pip install -q torch==2.0.1 torchvision==0.15.2 --extra-index-url https://download.pytorch.org/whl/cu118 xformers==0.0.21"]},{"cell_type":"markdown","metadata":{"id":"cTq1OnD0dGTn"},"source":["# **Data Collection and Aggregation**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"execution":{"iopub.execute_input":"2024-08-22T02:51:16.178372Z","iopub.status.busy":"2024-08-22T02:51:16.177321Z","iopub.status.idle":"2024-08-22T02:51:16.701477Z","shell.execute_reply":"2024-08-22T02:51:16.700131Z","shell.execute_reply.started":"2024-08-22T02:51:16.178323Z"},"id":"xCaVkbTVX4qI","outputId":"a36b6690-fcc7-4369-be5b-e519e3759469","trusted":true},"outputs":[{"data":{"text/plain":["label\n","म घर मा धेरै काम गर्छु ।          16\n","म संग धेरै पैसा छैन ।             12\n","मेरो साथी हरु भक्तपुर मा छन् ।    12\n","मेरो घर भक्तपुर मा छ ।             8\n","मेरो धेरै साथी हरु छन् ।           8\n","Name: count, dtype: int64"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import pandas as pd\n","\n","root_path = \"/kaggle/input/nepali-sign-lang\"\n","folder_list = os.listdir(root_path)\n","label_list = [path for path in folder_list if not path.endswith((\".csv\"))]\n","\n","total_df = pd.read_csv(os.path.join(root_path,\"train.csv\"))\n","\n","total_df.reset_index(drop = True, inplace = True)\n","total_df['label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"8WeH5kRLX4qJ"},"source":["# **Data Splitting**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-22T02:51:19.442079Z","iopub.status.busy":"2024-08-22T02:51:19.441646Z","iopub.status.idle":"2024-08-22T02:51:20.123160Z","shell.execute_reply":"2024-08-22T02:51:20.121692Z","shell.execute_reply.started":"2024-08-22T02:51:19.442043Z"},"id":"_m-QX6C8X4qJ","outputId":"3221ede6-6ecf-43fe-b466-53d8bff199cb","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes: ['म घर मा धेरै काम गर्छु ।', 'म संग धेरै पैसा छैन ।', 'मेरो घर भक्तपुर मा छ ।', 'मेरो धेरै साथी हरु छन् ।', 'मेरो साथी हरु भक्तपुर मा छन् ।'].\n","Splitted data: 44 12\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","def correct_file_path(file_name: str, root_path: str):\n","    return os.path.join(root_path, file_name)\n","\n","def preprocess_meta_df(df, root_path, label2id):\n","    df.rename(columns={\"video_name\": \"video_path\"}, inplace=True)\n","    df['video_path'] = df['video_path'].apply(lambda x: correct_file_path(x, root_path))\n","    df['label'] = df['label'].apply(lambda x: label2id[x])\n","    df['label'] = df['label']\n","    \n","    return df\n","\n","train_meta_df, test_meta_df = train_test_split(total_df, test_size=0.2, stratify=total_df['label'], random_state=42)\n","\n","label_list = list(set(train_meta_df['label']))\n","class_labels = sorted(label_list)\n","label2id = {label: i for i, label in enumerate(class_labels)}\n","id2label = {i: label for label, i in label2id.items()}\n","\n","print(f\"Unique classes: {list(label2id.keys())}.\")\n","\n","train_meta_df = preprocess_meta_df(train_meta_df, root_path, label2id)\n","test_meta_df = preprocess_meta_df(test_meta_df, root_path, label2id)\n","\n","print(\"Splitted data:\", len(train_meta_df), len(test_meta_df))"]},{"cell_type":"markdown","metadata":{"id":"0OSo11N6X4qK"},"source":["#  **Model Selection and Design**"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Configuration for the final Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_config():\n","    return {\n","        \"batch_size\": 1,\n","        \"num_epochs\": 2,\n","        \"lr\": 10**-4,\n","        \"seq_len\": 3136,\n","        \"d_model\": 768,\n","        \"lang_tgt\": \"ne\",\n","        \"model_folder\": \"weights\",\n","        \"model_basename\": \"tmodel_\",\n","        \"preload\": \"latest\",\n","        \"tokenizer_file\": \"tokenizer_{0}.json\",\n","        \"experiment_name\": \"/content/drive/MyDrive/English2Nepali/runs\"\n","        # \"experiment_name\": \"/content/drive/MyDrive/translation/runs\"\n","    }\n","\n","config=get_config()\n","device='cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-trained Encoder Model (Video Vision Transformer)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-08-22T02:59:07.706115Z","iopub.status.busy":"2024-08-22T02:59:07.705646Z","iopub.status.idle":"2024-08-22T02:59:08.671878Z","shell.execute_reply":"2024-08-22T02:59:08.670647Z","shell.execute_reply.started":"2024-08-22T02:59:07.706074Z"},"id":"UsdszFloX4qK","outputId":"8de48368-2efd-4503-8255-2fcae2b354a9","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","import pytorchvideo.data\n","from torch.utils.data import Dataset\n","from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n","from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n","\n","from pytorchvideo.transforms import (\n","    ApplyTransformToKey,\n","    Normalize,\n","    RandomShortSideScale,\n","    RemoveKey,\n","    ShortSideScale,\n","    UniformTemporalSubsample,\n",")\n","\n","from torchvision.transforms import (\n","    Compose,\n","    Lambda,\n","    RandomCrop,\n","    RandomHorizontalFlip,\n","    Resize,\n",")\n","\n","model_checkpoint = \"google/vivit-b-16x2-kinetics400\"\n","image_processor = VivitImageProcessor.from_pretrained(model_checkpoint)\n","\n","# model = VivitForVideoClassification.from_pretrained(model_checkpoint)\n","vivit_model=VivitModel.from_pretrained(model_checkpoint)"]},{"cell_type":"markdown","metadata":{},"source":["## Don't Need the Final Pooler Layer"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T02:59:18.972748Z","iopub.status.busy":"2024-08-22T02:59:18.972358Z","iopub.status.idle":"2024-08-22T02:59:18.985899Z","shell.execute_reply":"2024-08-22T02:59:18.984405Z","shell.execute_reply.started":"2024-08-22T02:59:18.972720Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VivitModel(\n","  (embeddings): VivitEmbeddings(\n","    (patch_embeddings): VivitTubeletEmbeddings(\n","      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n","    )\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n","  (encoder): VivitEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x VivitLayer(\n","        (attention): VivitAttention(\n","          (attention): VivitSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (output): VivitSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (intermediate): VivitIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (intermediate_act_fn): FastGELUActivation()\n","        )\n","        (output): VivitOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (pooler): Identity()\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#Replace pooler layer with the identity function, it just returns what it gets\n","\n","class Identity(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","    def forward(self, x):\n","        return x\n","    \n","\n","vivit_model.pooler=Identity()\n","# model.classifier=Identity()\n","# model.classifier=torch.nn.Linear(768,5)\n","vivit_model"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Decoder Model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T03:11:32.677732Z","iopub.status.busy":"2024-08-22T03:11:32.676811Z","iopub.status.idle":"2024-08-22T03:11:32.723996Z","shell.execute_reply":"2024-08-22T03:11:32.722475Z","shell.execute_reply.started":"2024-08-22T03:11:32.677680Z"},"trusted":true},"outputs":[],"source":["import math\n","\n","class InputEmbeddings(torch.nn.Module):\n","\n","    def __init__(self, d_model: int=768, vocab_size: int=20) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.embedding = torch.nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        # (batch, seq_len) --> (batch, seq_len, d_model)\n","        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n","        return self.embedding(x) * math.sqrt(self.d_model)\n","\n","\n","class PositionalEncoding(torch.nn.Module):\n","\n","    def __init__(self, d_model: int=768, seq_len: int=3136, dropout: float=0.1) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.dropout = torch.nn.Dropout(dropout)\n","        # Create a matrix of shape (seq_len, d_model)\n","        pe = torch.zeros(seq_len, d_model)\n","        # Create a vector of shape (seq_len)\n","        position = torch.arange(\n","            0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len, 1)\n","        # Create a vector of shape (d_model)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float(\n","        ) * (-math.log(10000.0) / d_model))  # (d_model / 2)\n","        # Apply sine to even indices\n","        # sin(position * (10000 ** (2i / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        # Apply cosine to odd indices\n","        # cos(position * (10000 ** (2i / d_model))\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        # Add a batch dimension to the positional encoding\n","        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n","        # Register the positional encoding as a buffer\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model)\n","        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n","        return self.dropout(x)\n","    \n","    \n","    \n","class MultiHeadAttentionBlock(torch.nn.Module):\n","\n","    def __init__(self, d_model: int=768, h: int=8, dropout: float=0.1) -> None:\n","        super().__init__()\n","        self.d_model = d_model  # Embedding vector size\n","        self.h = h  # Number of heads\n","        # Make sure d_model is divisible by h\n","        assert d_model % h == 0, \"d_model is not divisible by h\"\n","\n","        self.d_k = d_model // h  # Dimension of vector seen by each head\n","        self.w_q = torch.nn.Linear(d_model, d_model, bias=False)  # Wq\n","        self.w_k = torch.nn.Linear(d_model, d_model, bias=False)  # Wk\n","        self.w_v = torch.nn.Linear(d_model, d_model, bias=False)  # Wv\n","        self.w_o = torch.nn.Linear(d_model, d_model, bias=False)  # Wo\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","    @staticmethod\n","    def attention(query, key, value, mask, dropout: torch.nn.Dropout):\n","        d_k = query.shape[-1]\n","        # Just apply the formula from the paper\n","        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n","        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n","        if mask is not None:\n","            # Write a very low value (indicating -inf) to the positions where mask == 0\n","            attention_scores.masked_fill_(mask == 0, -1e9)\n","        # (batch, h, seq_len, seq_len) # Apply softmax\n","        attention_scores = attention_scores.softmax(dim=-1)\n","        if dropout is not None:\n","            attention_scores = dropout(attention_scores)\n","        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n","        # return attention scores which can be used for visualization\n","        return (attention_scores @ value), attention_scores\n","\n","    def forward(self, q, k, v, mask):\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        query = self.w_q(q)\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        key = self.w_k(k)\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        value = self.w_v(v)\n","\n","        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n","        query = query.view(\n","            query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n","        key = key.view(key.shape[0], key.shape[1],\n","                       self.h, self.d_k).transpose(1, 2)\n","        value = value.view(\n","            value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n","\n","        # Calculate attention\n","        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n","            query, key, value, mask, self.dropout)\n","\n","        # Combine all the heads together\n","        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n","        x = x.transpose(1, 2).contiguous().view(\n","            x.shape[0], -1, self.h * self.d_k)\n","\n","        # Multiply by Wo\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n","        return self.w_o(x)\n","    \n","\n","class LayerNormalization(torch.nn.Module):\n","\n","    def __init__(self, features: int, eps: float = 10**-6) -> None:\n","        super().__init__()\n","        self.eps = eps\n","        # alpha is a learnable parameter\n","        self.alpha = torch.nn.Parameter(torch.ones(features))\n","        # bias is a learnable parameter\n","        self.bias = torch.nn.Parameter(torch.zeros(features))\n","\n","    def forward(self, x):\n","        # x: (batch, seq_len, hidden_size)\n","        # Keep the dimension for broadcasting\n","        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n","        # Keep the dimension for broadcasting\n","        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n","        # eps is to prevent dividing by zero or when std is very small\n","        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n","\n","\n","class FeedForwardBlock(torch.nn.Module):\n","\n","    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n","        super().__init__()\n","        self.linear_1 = torch.nn.Linear(d_model, d_ff)  # w1 and b1\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.linear_2 = torch.nn.Linear(d_ff, d_model)  # w2 and b2\n","\n","    def forward(self, x):\n","        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n","        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n","\n","\n","class ResidualConnection(torch.nn.Module):\n","\n","    def __init__(self, features: int, dropout: float) -> None:\n","        super().__init__()\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.norm = LayerNormalization(features)\n","\n","    # many transformer implmentation also do like this--> normalize the input + positional embedding, then apply mhsa and add skip connection.\n","    # here sublayer is MHSA\n","    def forward(self, x, sublayer):\n","      return x + self.dropout(self.norm(sublayer(x)))\n","\n","\n","\n","class DecoderBlock(torch.nn.Module):\n","\n","    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.cross_attention_block = cross_attention_block\n","        self.feed_forward_block = feed_forward_block\n","        self.residual_connections = torch.nn.ModuleList(\n","            [ResidualConnection(features, dropout) for _ in range(3)])\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","        x = self.residual_connections[0](\n","            x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n","        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(\n","            x, encoder_output, encoder_output, src_mask))\n","        x = self.residual_connections[2](x, self.feed_forward_block)\n","        return x\n","\n","\n","class Decoder(torch.nn.Module):\n","\n","    def __init__(self, features: int, layers: torch.nn.ModuleList) -> None:\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNormalization(features)\n","\n","    def forward(self, x, encoder_output, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, encoder_output, src_mask, tgt_mask)\n","        return self.norm(x)\n","\n","\n","class ProjectionLayer(torch.nn.Module):\n","\n","    def __init__(self, d_model, vocab_size) -> None:\n","        super().__init__()\n","        self.proj = torch.nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x) -> None:\n","        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n","        return self.proj(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Building Video2Text Transformer Architecture"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T03:11:37.961825Z","iopub.status.busy":"2024-08-22T03:11:37.961273Z","iopub.status.idle":"2024-08-22T03:11:37.976530Z","shell.execute_reply":"2024-08-22T03:11:37.975095Z","shell.execute_reply.started":"2024-08-22T03:11:37.961762Z"},"trusted":true},"outputs":[],"source":["class Video2Text(torch.nn.Module):\n","\n","    def __init__(self, encoder, src_video, decoder: Decoder, tgt_embed: InputEmbeddings, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n","        super().__init__()\n","        self.video_encoder = encoder\n","        self.decoder = decoder\n","        self.src_video=src_video\n","        self.tgt_embed = tgt_embed\n","        self.tgt_pos = tgt_pos\n","        self.projection_layer = projection_layer\n","\n","    def encode(self):\n","    # (batch,num_frames, num_channels, height, width)\n","        perumuted_sample_test_video = self.src_video.permute(0,2, 1, 3, 4)\n","\n","        inputs = {\n","            \"pixel_values\": perumuted_sample_test_video,\n","        }\n","        # forward pass\n","        outputs = self.video_encoder(**inputs)\n","        return outputs.logits\n","\n","    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n","        # (batch, seq_len, d_model)\n","        tgt = self.tgt_embed(tgt)\n","        tgt = self.tgt_pos(tgt)\n","        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n","\n","    def project(self, x):\n","        # (batch, seq_len, vocab_size)\n","        return self.projection_layer(x)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T03:12:17.220311Z","iopub.status.busy":"2024-08-22T03:12:17.219851Z","iopub.status.idle":"2024-08-22T03:12:17.232403Z","shell.execute_reply":"2024-08-22T03:12:17.230974Z","shell.execute_reply.started":"2024-08-22T03:12:17.220276Z"},"trusted":true},"outputs":[],"source":["def build_transformer(encoder_model,src_video,tgt_vocab_size: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Video2Text:\n","    \n","    # Create the embedding layers\n","    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n","\n","    # Create the positional encoding layers\n","    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n","\n","    # Create the decoder blocks\n","    decoder_blocks = []\n","    for _ in range(N):\n","        decoder_self_attention_block = MultiHeadAttentionBlock(\n","            d_model, h, dropout)\n","        decoder_cross_attention_block = MultiHeadAttentionBlock(\n","            d_model, h, dropout)\n","        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n","        decoder_block = DecoderBlock(d_model, decoder_self_attention_block,\n","                                     decoder_cross_attention_block, feed_forward_block, dropout)\n","        decoder_blocks.append(decoder_block)\n","\n","    # Create the encoder and decoder\n","    video_encoder = encoder_model\n","    decoder = Decoder(d_model, torch.nn.ModuleList(decoder_blocks))\n","\n","    # Create the projection layer\n","    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n","\n","    # Create the transformer\n","    transformer = Video2Text(\n","        encoder=video_encoder,src_video=src_video, decoder=decoder,tgt_embed=tgt_embed, tgt_pos=tgt_pos, projection_layer=projection_layer)\n","\n","    # Initialize the parameters\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            torch.nn.init.xavier_uniform_(p)\n","\n","    return transformer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T03:12:20.616840Z","iopub.status.busy":"2024-08-22T03:12:20.615672Z","iopub.status.idle":"2024-08-22T03:12:20.622650Z","shell.execute_reply":"2024-08-22T03:12:20.621475Z","shell.execute_reply.started":"2024-08-22T03:12:20.616781Z"},"trusted":true},"outputs":[],"source":["def get_model(config,enc_model,src_video,vocab_tgt_len):\n","    v2t_model = build_transformer(encoder_model=enc_model,src_video=src_video, tgt_vocab_size=vocab_tgt_len,\n","                              tgt_seq_len=config['seq_len'], d_model=config['d_model'])\n","    return v2t_model"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-22T03:12:25.628962Z","iopub.status.busy":"2024-08-22T03:12:25.627684Z","iopub.status.idle":"2024-08-22T03:12:27.824207Z","shell.execute_reply":"2024-08-22T03:12:27.822804Z","shell.execute_reply.started":"2024-08-22T03:12:25.628913Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Video2Text(\n","  (video_encoder): VivitModel(\n","    (embeddings): VivitEmbeddings(\n","      (patch_embeddings): VivitTubeletEmbeddings(\n","        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): VivitEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x VivitLayer(\n","          (attention): VivitAttention(\n","            (attention): VivitSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): VivitSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): VivitIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","            (intermediate_act_fn): FastGELUActivation()\n","          )\n","          (output): VivitOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","    (pooler): Identity()\n","  )\n","  (decoder): Decoder(\n","    (layers): ModuleList(\n","      (0-5): 6 x DecoderBlock(\n","        (self_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=768, out_features=768, bias=False)\n","          (w_k): Linear(in_features=768, out_features=768, bias=False)\n","          (w_v): Linear(in_features=768, out_features=768, bias=False)\n","          (w_o): Linear(in_features=768, out_features=768, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (cross_attention_block): MultiHeadAttentionBlock(\n","          (w_q): Linear(in_features=768, out_features=768, bias=False)\n","          (w_k): Linear(in_features=768, out_features=768, bias=False)\n","          (w_v): Linear(in_features=768, out_features=768, bias=False)\n","          (w_o): Linear(in_features=768, out_features=768, bias=False)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (feed_forward_block): FeedForwardBlock(\n","          (linear_1): Linear(in_features=768, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear_2): Linear(in_features=2048, out_features=768, bias=True)\n","        )\n","        (residual_connections): ModuleList(\n","          (0-2): 3 x ResidualConnection(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (norm): LayerNormalization()\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNormalization()\n","  )\n","  (tgt_embed): InputEmbeddings(\n","    (embedding): Embedding(20, 768)\n","  )\n","  (tgt_pos): PositionalEncoding(\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (projection_layer): ProjectionLayer(\n","    (proj): Linear(in_features=768, out_features=20, bias=True)\n","  )\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["v2t_model=get_model(config,vivit_model,torch.randn(1,3,32,224,224),20)\n","v2t_model"]},{"cell_type":"markdown","metadata":{"id":"aiN2WpiMX4qK"},"source":["# **Apply Necessary Transform and Prepare Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGEtddR_X4qK","trusted":true},"outputs":[],"source":["class CustomVideoDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.dataframe = dataframe\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        video_path = row['video_path']\n","        label = row['label']\n","        return video_path, label\n","\n","mean = image_processor.image_mean\n","std = image_processor.image_std\n","\n","if \"shortest_edge\" in image_processor.size:\n","    height = width = image_processor.size[\"shortest_edge\"]\n","else:\n","    height = image_processor.size[\"height\"]\n","    width = image_processor.size[\"width\"]\n","\n","resize_to = (model.config.image_size, model.config.image_size)\n","\n","# num_frames_to_sample = model.config.num_frames\n","num_frames_to_sample = 32\n","clip_duration = 8\n","\n","train_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    RandomShortSideScale(min_size=256, max_size=320),\n","                    Resize(resize_to),\n","                    RandomHorizontalFlip(p=0.5),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","val_transform = Compose(\n","    [\n","        ApplyTransformToKey(\n","            key=\"video\",\n","            transform=Compose(\n","                [\n","                    UniformTemporalSubsample(num_frames_to_sample),\n","                    Lambda(lambda x: x / 255.0),\n","                    Normalize(mean, std),\n","                    Resize(resize_to),\n","                ]\n","            ),\n","        ),\n","    ]\n",")\n","\n","train_custom_dataset = CustomVideoDataset(train_meta_df)\n","train_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in train_custom_dataset]\n","\n","test_custom_dataset = CustomVideoDataset(test_meta_df)\n","test_labeled_video_paths = [(video_path, {'label': label}) for video_path, label in test_custom_dataset]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"Moy-JNAIX4qL","outputId":"db052e2b-041a-4b79-9ca2-25df1f7ebe5a","trusted":true},"outputs":[],"source":["import imageio\n","import numpy as np\n","from IPython.display import Image\n","\n","train_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =train_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n","    decode_audio=False,\n","    transform=train_transform,\n",")\n","\n","test_dataset = pytorchvideo.data.LabeledVideoDataset(\n","    labeled_video_paths =test_labeled_video_paths,\n","    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n","    decode_audio=False,\n","    transform=val_transform,\n",")\n","\n","def unnormalize_img(img):\n","    img = (img * std) + mean\n","    img = (img * 255).astype(\"uint8\")\n","    return img.clip(0, 255)\n","\n","def create_gif(video_tensor, filename=\"sample.gif\"):\n","    frames = []\n","    for video_frame in video_tensor:\n","        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n","        frames.append(frame_unnormalized)\n","    kargs = {\"duration\": 5}\n","    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n","    return filename\n","\n","def display_gif(video_tensor, gif_name=\"sample.gif\"):\n","    video_tensor = video_tensor.permute(1, 0, 2, 3)\n","    gif_filename = create_gif(video_tensor, gif_name)\n","    return Image(filename=gif_filename)\n","\n","sample_video = next(iter(train_dataset))\n","video_tensor = sample_video[\"video\"]\n","# print(id2label[sample_video['label']])\n","# display_gif(video_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["next(iter(train_dataset))['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CustomVideoDataset2(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer_tgt, tgt_lang, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.dataframe = dataframe\n","        self.tokenizer_tgt = tokenizer_tgt\n","        self.tgt_lang = tgt_lang\n","\n","        self.sos_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","#         return len(self.dataframe)\n","        return self.dataframe.num_videos\n","\n","    def __getitem__(self, idx):\n","        #new code here\n","        \n","        video=next(iter(self.dataframe))['video']\n","        label=next(iter(self.dataframe))['label']\n","#         target_txt=next(iter(self.dataframe))['label']\n","        \n","#         src_target_pair = self.ds[index]\n","#         src_text = src_target_pair[self.src_lang]\n","#         tgt_text = src_target_pair[self.tgt_lang]\n","\n","#         # Transform the output text into tokens\n","#         dec_input_tokens = self.tokenizer_tgt.encode(target_txt).ids\n","\n","#         # Add sos, eos and padding to each sentence\n","#         enc_num_padding_tokens = self.seq_len - \\\n","#             len(enc_input_tokens) - 2  # We will add <s> and </s>\n","\n","#         # We will only add <s> here, and </s> only on the label\n","#         dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n","\n","#         # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n","#         if dec_num_padding_tokens < 0:\n","#             raise ValueError(\"Sentence is too long\")\n","\n","#         # Add <s> and </s> token\n","#         encoder_input = torch.cat(\n","#             [\n","#                 self.sos_token,\n","#                 torch.tensor(enc_input_tokens, dtype=torch.int64),\n","#                 self.eos_token,\n","#                 torch.tensor([self.pad_token] *\n","#                              enc_num_padding_tokens, dtype=torch.int64),\n","#             ],\n","#             dim=0,\n","#         )\n","\n","         # Add only <s> token\n","#         decoder_input = torch.cat(\n","#             [\n","#                 self.sos_token,\n","#                 torch.tensor(dec_input_tokens, dtype=torch.int64),\n","#                 torch.tensor([self.pad_token] *\n","#                              dec_num_padding_tokens, dtype=torch.int64),\n","#             ],\n","#             dim=0,\n","#         )\n","\n","         # Add only </eos> token\n","#         label = torch.cat(\n","#             [\n","#                 torch.tensor(dec_input_tokens, dtype=torch.int64),\n","#                 self.eos_token,\n","#                 torch.tensor([self.pad_token] *\n","#                              dec_num_padding_tokens, dtype=torch.int64),\n","#             ],\n","#             dim=0,\n","#          )\n","\n","#         # Double check the size of the tensors to make sure they are all seq_len long\n","#         assert encoder_input.size(0) == self.seq_len\n","#         assert decoder_input.size(0) == self.seq_len\n","#         assert label.size(0) == self.seq_len\n","\n","        return {\n","#             \"encoder_input\": encoder_input,  # (seq_len)\n","#             \"decoder_input\": decoder_input,  # (seq_len)\n","#             # (1, 1, seq_len)\n","#             \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n","#             # (1, seq_len) & (1, seq_len, seq_len),\n","#             \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n","#             \"label\": label,  # (seq_len)\n","#             \"src_text\": src_text,\n","#             \"tgt_text\": tgt_text,\n","            \"video\":video,\n","            \"label\":label,\n","#             \"decoder_input\":decoder_input,\n","#             \"decoder_mask\":(decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n","#             \"tgt_text\":target_txt\n","            \n","        }\n","\n","def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tokenizers import Tokenizer\n","\n","target_tokenizer=Tokenizer.from_file(str('/kaggle/input/nepali-tokenizer/tokenizer_sign_lang_ne.json'))\n","\n","new_train_dataset=CustomVideoDataset2(train_dataset,target_tokenizer,'ne',20)\n","new_val_dataset=CustomVideoDataset2(test_dataset,target_tokenizer,'ne',20)\n","\n","# print(next(iter(new_train_dataset))['video'].shape,next(iter(new_train_dataset))['label'])\n","# print(next(iter(new_val_dataset))['video'].shape,next(iter(new_val_dataset))['label'])"]},{"cell_type":"markdown","metadata":{},"source":["# **Training Without Trainer**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader, random_split"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare and Test Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(new_train_dataset, batch_size=1)\n","val_dataloader = DataLoader(new_val_dataset, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for data in train_dataloader:\n","    print(data['video'].shape)\n","    break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data['video'].to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Optimizer and Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=10**-4, eps=1e-9)\n","loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for p in model.parameters():\n","#     p.requries_grad=False\n","    \n","# for p in model.classifier.parameters():\n","#     p.requires_grad=True"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","loss_list_train=[]\n","loss_list_val=[]\n","\n","for epoch in range(2):\n","    torch._C._cuda_emptyCache()\n","    model.train()\n","    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n","    acc_loss=0\n","    for batch in batch_iterator:\n","\n","    #             encoder_input = batch['encoder_input'].to(device)  # (b, seq_len)\n","    #             decoder_input = batch['decoder_input'].to(device)  # (B, seq_len)\n","    #             encoder_mask = batch['encoder_mask'].to(\n","    #                 device)  # (B, 1, 1, seq_len)\n","    #             decoder_mask = batch['decoder_mask'].to(\n","    #                 device)  # (B, 1, seq_len, seq_len)\n","\n","                # Run the tensors through the encoder, decoder and the projection layer\n","    #             encoder_output = model.encode(encoder_input, encoder_mask)  # (B, seq_len, d_model)\n","    #             decoder_output = model.decode(\n","    #                 encoder_output, encoder_mask, decoder_input, decoder_mask)  # (B, seq_len, d_model)\n","                # (B, seq_len, vocab_size)\n","    #             proj_output = model.project(decoder_output)\n","\n","                logits=run_inference(model,batch['video'].to(device))\n","                \n","                # Compare the output with the label\n","                label = batch['label'].to(device)  # (B, seq_len)\n","\n","                # Compute the loss using a simple cross entropy\n","                loss = loss_fn(logits, label.view(-1))\n","                acc_loss+=loss.item()\n","                # Log the loss\n","    #             writer.add_scalar('train loss', loss.item(), global_step)\n","    #             writer.flush()\n","                \n","        \n","                # Backpropagate the loss\n","                loss.backward()\n","\n","                # Update the weights\n","                optimizer.step()\n","                optimizer.zero_grad(set_to_none=True)\n","            \n","    loss_list_train.append(acc_loss/len(train_dataloader))\n","    batch_iterator.set_postfix({\"loss\": f\"{acc_loss/len(train_dataloader):6.3f}\"})\n","    \n","    \n","    model.eval()\n","    acc_loss=0\n","    for batchv in val_dataloader:\n","        with torch.no_grad():\n","            logits=run_inference(model,batchv['video'].to(device))\n","            label = batchv['label'].to(device)\n","            \n","            loss = loss_fn(logits, label.view(-1))\n","            acc_loss+=loss.item()\n","    \n","    loss_list_val.append(acc_loss/len(val_dataloader))\n","    \n","    print(f\"Training Loss: {loss_list_train[-1]}\")\n","    print(f\"Validation Loss: {loss_list_val[-1]}\")\n","                \n","                "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss_list_train[-1], loss_list_val[-1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSWdfgv7iTdS","trusted":true},"outputs":[],"source":["# trained_model = VivitForVideoClassification.from_pretrained('/kaggle/working/vivit-b-16x2-kinetics400-nepali_sign_lang/checkpoint-88')"]},{"cell_type":"markdown","metadata":{},"source":["# **Running Tests**"]},{"cell_type":"markdown","metadata":{},"source":["## Plot Each Frames Extracted from the Video"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Create subplots\n","fig, axs = plt.subplots(7, 7, figsize=(12, 12))\n","\n","# Plot images\n","f=0\n","for i in range(7):\n","    for j in range(7):\n","        f+=1\n","        if f<36:\n","          im=frame['video'].permute(1,0,2,3)[f,:,:,:].squeeze(0).permute(1,2,0)\n","          axs[i, j].imshow(im)\n","        axs[i, j].set_title(f'frame: {f}')\n","        axs[i, j].axis('off')  # Hide axis\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Inference on Single Video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1qsy-kql_AT","trusted":true},"outputs":[],"source":["def run_inference(model, video):\n","    \"\"\"Utility to run inference given a model and test video.\n","\n","    The video is assumed to be preprocessed already.\n","    \"\"\"\n","    # (batch,num_frames, num_channels, height, width)\n","    perumuted_sample_test_video = video.permute(0,2, 1, 3, 4)\n","\n","    inputs = {\n","        \"pixel_values\": perumuted_sample_test_video,\n","    }\n","    # forward pass\n","#     with torch.no_grad():\n","    outputs = model(**inputs)\n","    return outputs.logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z86pUo8gmys6","trusted":true},"outputs":[],"source":["\n","# Calling this function will only allow you to free unreferenced memory in cache\n","\n","# \n","# print(logits.shape)\n","model.eval()\n","with torch.no_grad():\n","    logits = run_inference(model.to(device), data['video'].to(device))\n","torch._C._cuda_emptyCache()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5551712,"sourceId":9184613,"sourceType":"datasetVersion"},{"datasetId":5567999,"sourceId":9208671,"sourceType":"datasetVersion"},{"datasetId":5568230,"sourceId":9209000,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
